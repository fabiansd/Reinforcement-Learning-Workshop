{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning using tables\n",
    "##### Authors: Eirik Fagtun KjÃ¦rnli and Fabian Dietrichson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question for participants during the workshop\n",
    "\n",
    "#### Questions for us\n",
    "- How complex can each task be?\n",
    "- Which visualizations do we need?\n",
    "- How to make sure progress is upheld during the workshop?\n",
    "- How to make sure everybody manages to follow?\n",
    "\n",
    "Possible environments\n",
    "- taxi-v2 (https://medium.com/@anirbans17/reinforcement-learning-for-taxi-v2-edd7c5b76869)\n",
    "\n",
    "#### Links\n",
    "Q-learning and introduction to the algorithm\n",
    "- Inspo: https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to Jupyter Notebooks and structure of notebook\n",
    "This workshop is structured such that for each cell you will write your code between .. blabal..\n",
    "\n",
    "### Task:\n",
    "Click on the cell below, and run it to import the necessary libaries\n",
    "Hot key to run a cell: ctrl + enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_input_by_2(a_variabel):\n",
    "\n",
    "    \"Write code below\" \n",
    "    result = a_variabel * 2\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(multiply_input_by_2(10) == 20), \"Your method did not multiple the input by 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of neccesary packages\n",
    "The first step to import the necessary packages. The gym package is the most central which contains the games which our Reinforcement learning agents are going to solve.\n",
    "\n",
    "#### Task:\n",
    "Click on the cell below and run it, to import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce the game we are going to solve here\n",
    "\n",
    "Blabla this is how the game works\n",
    "\n",
    "#### Task:\n",
    "Create the environment variabel containing all necessary methods to run the Taxi-v2 game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Introduce the game we are going to solve here\n",
    "env = gym.make(\"Taxi-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Q-table\n",
    "Q-learning is one the most iconic Reinforcement learning algorihms, and can be used to solve a great variaty of challenges. In its most simplistic form it uses a table to store its values.\n",
    "\n",
    "Each row in the array is a state\n",
    "Each column is an action\n",
    "\n",
    "#### Task:\n",
    "Create and return a Q-table, where each cell is initialized to zeros, with:\n",
    "- The number of columns equal to number of actions in the environment (Use env.action_space.n)\n",
    "- The number of rows equal to number of states in the environment (Use env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_table(env):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    action_size = env.action_space.n\n",
    "    observation_size = env.observation_space.n\n",
    "    \n",
    "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "assert(np.count_nonzero(create_q_table(env) == 0)), \"All values in Q-table should be zero\"\n",
    "assert(create_q_table(env).shape == (500,6)), \"The dimensions are wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best action\n",
    "The next method will pick out the best action given the state\n",
    "\n",
    "#### Task\n",
    "Pick the action with the highest Q-value given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_best_action(Q, state):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    best_action =  np.argmax(Q[state,:]) \n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "Q_temp = create_mock_q_table(env)\n",
    "assert(get_best_action(Q_temp, 50) == 0), \"The method did not pick the action with the highest Q_value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an action\n",
    "The exploration vs. exploitation is a very effective method to ensure the agent explores a sufficent area of the state space, and avoid coverging to a local optima.\n",
    "\n",
    "#### Task:\n",
    "Compute the action to take in the current state, including exploration.  \n",
    "If the probability, e.g. epsilon, is higher than a random number, we should take a random action\n",
    "    otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "Tip: \n",
    "- To pick a random action, use env.action_space.sample()\n",
    "- To generate a random number with uniform probability, use np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_action(Q, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    if epsilon > np.random.uniform():\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = get_best_action(Q, state)\n",
    "    \n",
    "    \"Write code above\"    \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q_temp = create_mock_q_table(env)\n",
    "assert(select_action(Q_temp, 50, 0) == 0), \\\n",
    "\"Method should always return the same value for this state, since epsilon is 0\"\n",
    "assert not (len(set([select_action(Q_temp, 50, 1) for x in range(20)])) <= 2), \\\n",
    "\"Method should not return identical values when a random action, should be chosen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradually shift towards exploitation, and reduce exploration of state space\n",
    "To make sure we are gradually moving from exploring the environment by taking random actions, we need to reduce the possibility of choosing a random action. In other words, we need to reduce Epsilon. \n",
    "\n",
    "#### Task:\n",
    "Create a method which reduces epsilon by a factor called epsilon decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_epsilon(epsilon):\n",
    "    epsilon_decay = 0.95\n",
    "    \n",
    "    \"Write code below\" \n",
    "    epsilon *= 0.95\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(reduce_epsilon(5) == 4.75), \"Given an input of 5, the output should have been 4.75\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Q-table\n",
    "\n",
    "Q-learning equation\n",
    "- Write equation here\n",
    "\n",
    "#### Task\n",
    "You are to implement the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Short explaination of q-table algorithm\n",
    "\"\"\"\n",
    "def update_q_table(Q, action, state, done, reward, new_state):\n",
    "    \n",
    "    if(done):\n",
    "        # Since it is the final state\n",
    "        Q[state,action] = reward\n",
    "    else:\n",
    "        # Error in estimate is not a completly correct name due to the discount factor...\n",
    "        error_in_estimate = reward + discount_factor*np.max(Q[new_state,:]) - Q[state,action]\n",
    "        Q[state, action] = Q[state, action] + learning_rate*error_in_estimate\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Methods we should create?\n",
    "def visualize_q_table():\n",
    "    # See if we find some good ways to visualize the q-table\n",
    "    yield\n",
    "\n",
    "def store_data_for_visualization():\n",
    "    # To plot reward development\n",
    "    # Number of iterations\n",
    "    yield\n",
    "\n",
    "def reset_env_and_update_params(env, epsilon):\n",
    "    epsilon_decay = 0.99;\n",
    "    epsilon *= epsilon_decay\n",
    "    \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    return state, done, iterations, total_reward, epsilon\n",
    "\n",
    "def render_performance(Q):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Get action\n",
    "        action = np.argmax(Q[state,:]) \n",
    "\n",
    "        # Take action in environment\n",
    "        new_state, _, done, _ = env.step(action)\n",
    "\n",
    "        # Update current state\n",
    "        state =  new_state\n",
    "\n",
    "        # Render\n",
    "        env.render()\n",
    "        \n",
    "def create_mock_q_table(env):\n",
    "    Q = create_q_table(env)\n",
    "    Q[50,0] = 1\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulation\n",
    "num_episodes = 1000\n",
    "max_iterations = 40\n",
    "\n",
    "# Q-learning parameters\n",
    "discount_factor = 0.95\n",
    "learning_rate = 0.8\n",
    "epsilon = 1\n",
    "done = False\n",
    "\n",
    "# Initiallize environment and create Q-table\n",
    "Q = create_q_table(env)\n",
    "\n",
    "# Run simulation\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    if (done and episode % 50 == 0): \n",
    "        print(\"Episode: {} | Iterations: {} | Total Reward: {}\".format(episode, iterations, total_reward))\n",
    "    state, done, iterations, total_reward, epsilon = reset_env_and_update_params(env, epsilon)\n",
    "        \n",
    "    while not done:\n",
    "        # Get action\n",
    "        action = select_action(state, epsilon)\n",
    "        \n",
    "        # Take action in environment\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Update Q-table\n",
    "        Q = update_q_table(Q, action, state, done, reward, new_state)\n",
    "        \n",
    "        # Update current state\n",
    "        state = new_state\n",
    "        \n",
    "        # End check\n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        if (iterations >= max_iterations): done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "render_performance(Q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
