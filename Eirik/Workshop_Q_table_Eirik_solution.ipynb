{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning using tables\n",
    "##### Authors: Eirik Fagtun Kjærnli and Fabian Dietrichson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome \n",
    "This workshop is structured such that for each cell, you will write your code own code and the code will then be asserted in the next cell. The assertion cell is marked \"# Do not edit - Assertion cell #\".\n",
    "\n",
    "The code should be written between \"Write code below\" and \"Write code above\". If your code do not pass the assertion, you will have to rewrite your code before continuing.\n",
    "\n",
    "### Task:\n",
    "Click on the cell below, and run it to import the necessary libaries\n",
    "Hot key to run a cell: CTRL + ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_input_by_2(a_variabel):\n",
    "\n",
    "    \"Write code below\" \n",
    "    result = a_variabel * 2\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, you correctly implemented the method!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(multiply_input_by_2(10) == 20), \"Your method did not multiple the input by 2\"\n",
    "print(\"Great, you correctly implemented the method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of neccesary packages\n",
    "To implement the native Q-learning algorithm and use it in a environment, we just need two additional packages.\n",
    "\n",
    "_Numpy_\n",
    "- Numpy adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. We could do this workshop using native Python arrays and built-in methods, however using Numpy simplifies the process. \n",
    "- To simply how we call the methods in the package, we use the \"as np\" command. As a result, you can simply write np.zeros(2, 2) to create a numpy table with 2 rows and 2 columns, where all the cells are zero.\n",
    "\n",
    "_OpenAI Gym_\n",
    "- The Gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. This range from simple text problems, to complex physisical problems, to Atari video games. This has made OpenAI Gym the prefeered framework to learn and test Reinforcement learning algorithms.\n",
    "\n",
    "\n",
    "### Task:\n",
    "To import the necessary packages, simply run the cell below, and then run the assertion cell to verify that they have been imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, the packages were imported correctly!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(gym)\n",
    "    print(\"Great, the packages were imported correctly!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce the game we are going to solve here\n",
    "\n",
    "The environment we are going to use is a simple grid world, where the agent is controlling a taxi. The environment was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop the passenger off in another. You can read more about the environment here: https://gym.openai.com/envs/Taxi-v2/\n",
    "\n",
    "### Task:\n",
    "Create the environment variabel containing all necessary methods to run the Taxi-v2 game. <br>\n",
    "_Tip: Just run the cell below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_id = \"Taxi-v2\"\n",
    "env = gym.make(environment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You successfully created the environement Taxi-v2\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(env)\n",
    "    print(\"You successfully created the environement {}\".format(env.spec.id))\n",
    "except:\n",
    "    print(\"You create the environment incorrectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in additon to support function which will be used throughout the workshop. \n",
    "\n",
    "### Task\n",
    "Do as you have done before, simply mark the cell below and press CTRL + Enter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TestEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.reset()\n",
    "        \n",
    "    def play(self, action):\n",
    "        if not (0 <= int(action) <= 5):\n",
    "            print(\"Action value must either be 0,1,2,3,4 or 5\")\n",
    "            return\n",
    "\n",
    "        clear_output()\n",
    "        _, reward, done, _ = self.env.step(action)\n",
    "        self.env.render()\n",
    "        print(\"Reward: \", reward)\n",
    "        \n",
    "        if(done):\n",
    "            print(\"Game completed, resetting environment!\")\n",
    "            self.reset_env()\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.env.reset()\n",
    "        print(\"Environment has been reset\")\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "\n",
    "class MockData():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.seed(10)\n",
    "        \n",
    "    def get_Q(self):\n",
    "        Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q[0:2,0] = 10\n",
    "        return Q\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "\n",
    "def visualize_q_table():\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "    heat_map = sb.heatmap(Q_trained)\n",
    "    plt.show()\n",
    "\n",
    "def plot_visualization(data):\n",
    "    \n",
    "    reward_list = data[0]\n",
    "    iteration_list = data[1]\n",
    "    epsilon_list = data[2]\n",
    "    \n",
    "    episodes = range(len(reward_list))\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(311)\n",
    "    ax.set_title(\"Reward\")\n",
    "    ax = plt.plot(episodes, reward_list)\n",
    "\n",
    "    ax = plt.subplot(312)\n",
    "    ax.set_title(\"Iterations per episode\")\n",
    "    ax.plot(episodes, iteration_list)\n",
    "    \n",
    "    ax = plt.subplot(313)\n",
    "    ax.set_title(\"Epsilon\")\n",
    "    ax.set_xlabel(\"Episodes\")\n",
    "    ax.plot(episodes, epsilon_list)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def reset_env_and_update_params(env):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    return state, done, iterations, total_reward\n",
    "\n",
    "def render_performance(env, Q):\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    t_sleep = 1.2\n",
    "    \n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    time.sleep(t_sleep)\n",
    "    clear_output()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state,:]) \n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        state =  new_state\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(t_sleep)\n",
    "        clear_output()\n",
    "        \n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        if (iterations >= 20): \n",
    "            done = True\n",
    "            print(\"Agent did not complete the episode within 20 iterations, train your agent better!\")\n",
    "            return\n",
    "    \n",
    "    print(\"Your agent completed the task using {} iterations, \\\n",
    "          and got a total reward of {}\".format(iterations, total_reward))\n",
    "    \n",
    "test_env = TestEnvironment()\n",
    "current_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "mock = MockData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great, the support methods were created!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(mock)\n",
    "    print(\"Great, the support methods were created!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with environment\n",
    "Before we start creating the Q-learning agent, we will explore the environment we are going to use. This is done using a support method we have created for you.\n",
    "\n",
    "**How to play**\n",
    "1. Simply input an action, e.g. the integer 1, where it is indicated in the cell below, and click CTRL + Enter. \n",
    "2. The game will reset when you have picked up and then dropped off the passenger at the indicated location.\n",
    "3. If you would like to reset the test environment during the execution, run the cell which indicates this below. \n",
    "\n",
    "*Tip*: The bold colored letter shows where the passenger should be picked up, and the normal colored letter shows drop of spot.\n",
    "\n",
    "**Possible actions**\n",
    "0. Move down\n",
    "1. Move up\n",
    "2. Move right\n",
    "3. Move left\n",
    "4. Pick up passenger\n",
    "5. Drop off passenger\n",
    "\n",
    "### Task:\n",
    "Play around with the environment to understand the rewards and how the game dynamics works. When you will confident, move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to reset the environment\n",
    "test_env.reset_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "Reward:  -1\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to play the game\n",
    "\n",
    "# Input your action below\n",
    "action = 0\n",
    "# Input your action above\n",
    "\n",
    "test_env.play(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion point\n",
    "The game above is a simply environment, which is easy for humans to solve. However, if you were to use traditional programming, how would you solve it? \n",
    "\n",
    "### Task\n",
    "Use 5 minutes in the group to come up with a strategy, and create a quick draft!\n",
    "\n",
    "<img src=\"images/Discussion.jpg\" alt=\"drawing\" width=\"400\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "In Reinforcement learning an agent moves from a state $S_{t}$, to a new state $S_{t+1}$ by taking an action $A_{t}$, and recieving the reward $R_{t}$. The agent will repeat this prosess for a defined amount of iterations, and this process as a whole is known as an episode. The process is illustrated below. The goal of the Reinforcement learning agent is to maximize the total reward it is able to collect during an episode. To improve its performance, it learns from each state transition, i.e. move from $S_{t}$ to $S_{t+1}$. How it learns, is what seperates the different Reinforcement learning algorithms. \n",
    "\n",
    "<img src=\"images/Agent.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "In this notebook we will work with one of the most iconic Reinforcement learning algorihm in its simplest form, namely Q-learning using Q-tables. Q-tables referes to the fact that we are storing the agent knowledge of the best strategy in a table, i.e. the strategy which gives us the highest reward. The table can be thought of as the brain of the agent. In the table each row represents a state, and the columns represent the possible actions in that state. This will be explained further in the next section.\n",
    "\n",
    "A Q-value is a measure of the total expected reward the agent will recieve if it chooses that action, and always picks the action with the highest Q-value in the suceeding states, using its current knowledge of the environment, i.e. the Q-table. Simply said, the higher the Q-value the better the agent believes the action is. \n",
    "\n",
    "The Q-learning algorithm is used to update the Q-values for a given action $A_{t}$ in the state $S_{t}$, and is updated after each iteration. The equation is shown below.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "A key point to note in this equation, is that we are updating the Q(s,a) by using the the highest Q-value in the next state, $S_{t+1}$. This value is unknown, and it is therefore just the agent estimate of that state. This makes it an optimization problem were we gradually correct each Q-value as we recieve an reward moving between the states.\n",
    "\n",
    "\n",
    "This might seem daunting at first, however we will through this notebook break it into simple peices, which will hopefully, make it easier to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Q-table\n",
    "The Q-table is the brain of agent, and stores the agent's current knowledge of the Q-values of each state. Each row represents a state, and the columns represents all possible actions in that state. As a result, our table will have the dimmensions, (rows=number of states, columns=number of actions). An illustration of the table is shown below, altough without the labels on the columns. \n",
    "\n",
    "<img src=\"images/Taxi_matrix_initial.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "### Task:\n",
    "Create and return a Q-table using numpy, where each cell is initialized to zeros:\n",
    "- Create the Q-table by using the numpy command: np.zeros(rows, columns). <br>\n",
    "    https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\n",
    "- Use the following command to get the number of states in the environment:\n",
    "    - env.observation_space.n\n",
    "- Use the following command to get the number of possible actions. PS. all states have the same amount of actions.\n",
    "    - env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_table(env):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    action_size = env.action_space.n\n",
    "    observation_size = env.observation_space.n\n",
    "    \n",
    "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Q-table was correctly built! It has 500 rows, each representing a unique state, and 6 columns, each representing an action for that state.\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(np.count_nonzero(mock.get_Q() == 0)), \"All values in Q-table should be zero\"\n",
    "assert(mock.get_Q().shape == (mock.get_env().observation_space.n, mock.get_env().action_space.n)), \\\n",
    "\"The dimensions are wrong\"\n",
    "print(\"The Q-table was correctly built! \" +\n",
    "      \"It has {} rows, each representing a unique state, and {} columns, each representing an action for that state.\"\\\n",
    "      .format(mock.get_env().observation_space.n, mock.get_env().action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best action\n",
    "Before we begin implementing the Q-learning algorithm, we will implement a method which makes the agent pick the action with the highest Q-value, given a state.\n",
    "\n",
    "### Task\n",
    "Return the column index of the highest Q-value in the Q-table, given a state.\n",
    "- Tip: Use the argmax function from the numpy library. <br> https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(Q_table, state):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    best_action =  np.argmax(Q_table[state,:]) \n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best action for the test state was chosen, excellent!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(get_best_action(mock.get_Q(), 1) == 0), \"The method did not pick the action with the highest Q_value\"\n",
    "print(\"The best action for the test state was chosen, excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an action\n",
    "\n",
    "When we create the Q-table all Q-values are set to zero. As we begin to move around the environment, it is that we end up in a loop having only explored a small part of the environment. This is a problem so big that is has its own name; the Exploration vs Exploiation dilemma!\n",
    "\n",
    "One simply and highly solution is simply, and is used through \n",
    "The exploration vs. exploitation is a very effective method to ensure the agent explores a sufficent area of the state space, and avoid coverging to a local optima.\n",
    "\n",
    "### Task:\n",
    "Create method which picks a random action, IF a random value in the range [0,1] is less than epsilon, or ELSE picks the best action using the method we created earlier.   \n",
    "- To pick a random action, use env.action_space.sample()\n",
    "- To generate a random number with uniform probability in the range [0,1], use np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q_table, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    if epsilon > np.random.uniform():\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = get_best_action(Q_table, state)\n",
    "    \n",
    "    \"Write code above\"    \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct actions were picked, great job!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(select_action(mock.get_Q(), 1, 0) == 0), \\\n",
    "\"Method should always return the same value for this state, since epsilon is 0\"\n",
    "assert not (len(set([select_action(mock.get_Q(), 1, 1) for x in range(20)])) <= 2), \\\n",
    "\"Method should not return identical values when a random action, should be chosen\"\n",
    "print(\"The correct actions were picked, great job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradually shift towards exploitation, and reduce exploration of state space\n",
    "To make sure we are gradually moving from exploration to exploiation, we need to reduce the possibility of choosing a random action. In other words, we need to reduce Epsilon. This is done by multiplying it with a constant called epsilon decay.\n",
    "\n",
    "### Task:\n",
    "Create a method which reduces epsilon by multiplying itself with the the defined constant epsilon decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon(epsilon):\n",
    "    epsilon_decay = 0.95\n",
    "    \n",
    "    \"Write code below\" \n",
    "    epsilon *= epsilon_decay\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon was correctly updated!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(update_epsilon(5) == 4.75), \"Given an input of 5, the output should have been 4.75\"\n",
    "print(\"Epsilon was correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Find the highest Q_value for a given state\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ {\\color{red}{\\underset{a}{max} \\  Q(s_{t+1},a)}} - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}\n",
    "$ \n",
    "\n",
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_Q_value_in_state(Q_table, new_state):\n",
    "    \"Write code below\"\n",
    "    best_Q_value = np.max(Q_table[new_state,:])\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perfect, the method returned the highest Q-value for that state!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(find_highest_Q_value_in_state(mock.get_Q(), 1) == 10), \\\n",
    "\"The method did not pick the action with the highest Q_value\"\n",
    "print(\"Perfect, the method returned the highest Q-value for that state!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate error  in Q-value\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[{\\color{red}{r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})}}\\big]\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Need to explain the discount factor\n",
    "\n",
    "### Task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_in_Q_value(Q_table, state, action, reward, new_state):\n",
    "    \n",
    "    discount_factor = 0.95\n",
    "    \n",
    "    \"Write code below\"\n",
    "    highest_q_value_in_state = find_highest_Q_value_in_state(Q_table, new_state)\n",
    "    \n",
    "    error_in_Q_value = reward + discount_factor * highest_q_value_in_state - Q_table[state,action]\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return error_in_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error in Q-value have been calculated correctly!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(calculate_error_in_Q_value(mock.get_Q(), 0, 0, 10, 1) == 9.5), \\\n",
    "\"The method did not calculate the correct error value for the test sample\"\n",
    "print(\"The error in Q-value have been calculated correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Q-table\n",
    "\n",
    "Finaly we can put it all together and end up with the final equation.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow {\\color{red}{Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "### Task\n",
    "Create a method which either sets\n",
    " - The Q-value equal to the reward, if it was the final episode. (Done is true)\n",
    " - Else updates the Q by following the equation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(Q_table, state, action, done, reward, new_state):\n",
    "    \n",
    "    learning_rate = 0.8\n",
    "    \n",
    "    \"Write code below\"\n",
    "    if(done):\n",
    "        Q_table[state, action] = reward\n",
    "    else:\n",
    "        Q_table[state, action] = Q_table[state, action] + \\\n",
    "        learning_rate * calculate_error_in_Q_value(Q_table, state, action, reward, new_state)\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-tabel have been correctly updated!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(update_q_table(mock.get_Q(), 2, 1, 1, 10, 3)[2,1] == 10), \\\n",
    "\"The updated Q-value when simulation was done was incorrect\"\n",
    "assert(update_q_table(mock.get_Q(), 0, 0, 0, 10, 1)[0,0] == 17.6), \\\n",
    "\"The updated Q-value when the episode was not done, was incorrect\"\n",
    "print(\"Q-tabel have been correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take one step\n",
    "Take all we have learnt until this point\n",
    "\n",
    "### Task\n",
    "    Chose action\n",
    "    Make step\n",
    "    Return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(Q_table, env, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    action = select_action(Q_table, state, epsilon)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return action, reward, done, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your step method seems to function properly, good job!\n"
     ]
    }
   ],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "mock_env1 = deepcopy(mock.get_env())\n",
    "mock_env1.reset()\n",
    "mock_env2 = deepcopy(mock_env1)\n",
    "\n",
    "new_state, _, _, _ = mock_env1.step(0)\n",
    "assert(step(mock.get_Q(), mock_env2, 0, 0) == (0, -1, False, new_state)), \\\n",
    "\"Your environment did not return the correct values\"\n",
    "print(\"Your step method seems to function properly, good job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train method\n",
    "\n",
    "Explain what an episode and iteration is\n",
    "Explain code already writen\n",
    "\n",
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, num_episodes, max_iterations):\n",
    "    done = False\n",
    "    epsilon = 1\n",
    "    reward_list, iterations_list, epsilon_list = [], [], []\n",
    "    \n",
    "    \"Write code below\"\n",
    "    # Initialize Q-table\n",
    "    Q_table = create_q_table(env)\n",
    "    \"Write code above\"\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        if (done and episode % 10 == 0): \n",
    "            print(\"Episode: {} | Iterations: {} | Total Reward: {}\".format(episode, iterations, total_reward))\n",
    "        \n",
    "        state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "        \n",
    "        \"Write code below\"\n",
    "        # Update epsilon\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "        \n",
    "        \"Write code above\"\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            \"Write code below\"\n",
    "            \n",
    "            # Take step\n",
    "            action, reward, done, new_state = step(Q_table, env, state, epsilon)\n",
    "                        \n",
    "            # Update Q-table\n",
    "            Q_table = update_q_table(Q_table, state, action, done, reward, new_state)\n",
    "            \n",
    "            # Set the new state as the current state\n",
    "            state = new_state\n",
    "            \n",
    "            \"Write code above\"\n",
    "            \n",
    "            # Update episode information\n",
    "            iterations += 1\n",
    "            total_reward += reward\n",
    "            \n",
    "            # End episode if it has lasted for too many iterations\n",
    "            if (iterations >= max_iterations): done = True\n",
    "\n",
    "        reward_list.append(total_reward)\n",
    "        iterations_list.append(iterations)\n",
    "        epsilon_list.append(epsilon)\n",
    "        \n",
    "    return Q_table, (reward_list, iterations_list, epsilon_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create play method\n",
    "\n",
    "\n",
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, Q_table):\n",
    "    \n",
    "    state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        \"Write code below\"\n",
    "        # Take step\n",
    "        action, reward, done, new_state = step(Q_table, env, state, 0)\n",
    "\n",
    "        # Set the new state as the current state\n",
    "        state = new_state\n",
    "\n",
    "        \"Write code above\"\n",
    "\n",
    "        # Update episode information\n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        if (iterations >= 50): done = True\n",
    "    \n",
    "    print(\"Iterations: {} | Total Reward: {}\".format(iterations, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time to put our methods to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_episodes = 0\n",
    "maximum_iterations = 0\n",
    "\n",
    "Q_table_trained, data = train(env, maximum_episodes, maximum_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot key variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAJcCAYAAADdFyE9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdf5BdZ33n+fcHyQYSGztBDQuSbDljOUFx2JjtOM6yWVxlyEjORJoKhFgVB8x6UGUqIiR42ZiBMYyZZAIkkGUifsiLY3ACRjhZb08QoyRgIGEtj9rjxIXsmOqVMZJs4rYxDmDAFnz3j3NMrpuW+trn9r1X3e9XlarOj6fP+d77qLs//ZznnpOqQpIkSU/OU0ZdgCRJ0vHMMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYk6SiSXJPkP466DknjzTAlaSSSfDHJN5N8PcmX2+By0qjrkqQnyjAlaZR+oapOAn4SOAd4wyiKSLJyFOeVtDQYpiSNXFV9GdhDE6pI8tQkv5/kS0n+Mcn7kjy93feZJC9tl1+YpJL8fLt+QZK/a5f/RZJPJXkgyf1J/jTJqY+dsx0Z++0ktwHfSLIyyTlJ/nuSryX5KPC04b4Tko5HhilJI5dkDbAJmGk3/R5wFk24OhNYDVzR7vsMcH67/CLgAPC/9qx/5rHDAv8JeC7wPGAt8JY5p94K/DxwKs3PwxuAa4EfBj4GvLT7q5O01BmmJI3SDUm+BhwE7gPenCTANuC3quorVfU14HeBi9qv+QxNaIImRP2nnvXvhamqmqmqv6qqb1fVLPDOnnaPeXdVHayqbwLnAScAf1hVj1bV9cC+RXjNkpYYw5SkUfrXVXUyzUjTjwGrgAngB4Bbknw1yVeB/9puB7gJOCvJs2lGrj4ErE2yCjgX+CxAkmcnuS7J4ST/BPxJe/xeB3uWnwscrsc//f3uwb1USUuVYUrSyFXVZ4BrgN8H7ge+Cfx4VZ3a/julnahOVT0M3AK8Fvh8VT0C/L/A64D/r6rubw/7u0ABP1FVzwAuprn097hT9yzfC6xuR8Yec9oAX6akJcowJWlc/CHwEuAngKuAdyV5FkCS1Un+ZU/bzwDb+ef5UZ+esw5wMvB14KEkq4HXL3D+m4AjwG8kOSHJL9KMdEnSMRmmJI2Fdl7Th2gmmv82zWT0ve0lur8GfrSn+WdowtJnj7IO8B+AFwAPAR8H/nyB8z8C/CJwCfAV4JcX+hpJAsjjpwdIkiTpiXBkSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR2M7OGeq1atqnXr1o3q9JIkSX275ZZb7q+qifn2jSxMrVu3junp6VGdXpIkqW9JjvpEBC/zSZIkdWCYkiRJ6mDBMJXk6iT3Jfn8UfYnybuTzCS5LckLBl+mJEnSeOpnZOoaYOMx9m8C1rf/tgHv7V6WJEnS8WHBMFVVn6V5TtXRbAE+VI29wKlJnjOoAiVJksbZIOZMrQYO9qwfard9nyTbkkwnmZ6dnR3AqSVJkkZrqBPQq2pnVU1W1eTExLy3apAkSTquDCJMHQbW9qyvabdJkiQteYMIU1PAK9pP9Z0HPFRV9w7guJIkSWNvwTugJ/kIcD6wKskh4M3ACQBV9T5gN3AhMAM8DLxqsYqVJEkaNwuGqarausD+An59YBVJkiQdR7wDuiRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkddBXmEqyMcmdSWaSXD7P/tOS3Jjk1iS3Jblw8KVKkiSNnwXDVJIVwA5gE7AB2Jpkw5xmbwJ2VdU5wEXAewZdqCRJ0jjqZ2TqXGCmqg5U1SPAdcCWOW0KeEa7fApwz+BKlCRJGl/9hKnVwMGe9UPttl5vAS5OcgjYDbxmvgMl2ZZkOsn07OzskyhXkiRpvAxqAvpW4JqqWgNcCFyb5PuOXVU7q2qyqiYnJiYGdGpJkqTR6SdMHQbW9qyvabf1uhTYBVBVNwFPA1YNokBJkqRx1k+Y2gesT3JGkhNpJphPzWnzJeACgCTPowlTXseTJElL3oJhqqqOANuBPcAdNJ/a25/kyiSb22aXAa9O8vfAR4BLqqoWq2hJkqRxsbKfRlW1m2Ziee+2K3qWbwdeONjSJEmSxp93QJckSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgd9hakkG5PcmWQmyeVHafPyJLcn2Z/kw4MtU5IkaTytXKhBkhXADuAlwCFgX5Kpqrq9p8164A3AC6vqwSTPWqyCJUmSxkk/I1PnAjNVdaCqHgGuA7bMafNqYEdVPQhQVfcNtkxJkqTx1E+YWg0c7Fk/1G7rdRZwVpLPJdmbZON8B0qyLcl0kunZ2dknV7EkSdIYGdQE9JXAeuB8YCtwVZJT5zaqqp1VNVlVkxMTEwM6tSRJ0uj0E6YOA2t71te023odAqaq6tGqugv4Ak24kiRJWtL6CVP7gPVJzkhyInARMDWnzQ00o1IkWUVz2e/AAOuUJEkaSwuGqao6AmwH9gB3ALuqan+SK5NsbpvtAR5IcjtwI/D6qnpgsYqWJEkaF6mqkZx4cnKypqenR3JuSZKkJyLJLVU1Od8+74AuSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQO+gpTSTYmuTPJTJLLj9HupUkqyeTgSpQkSRpfC4apJCuAHcAmYAOwNcmGedqdDLwWuHnQRUqSJI2rfkamzgVmqupAVT0CXAdsmafdW4G3Ad8aYH2SJEljrZ8wtRo42LN+qN32PUleAKytqo8f60BJtiWZTjI9Ozv7hIuVJEkaN50noCd5CvBO4LKF2lbVzqqarKrJiYmJrqeWJEkauX7C1GFgbc/6mnbbY04GzgY+neSLwHnAlJPQJUnSctBPmNoHrE9yRpITgYuAqcd2VtVDVbWqqtZV1TpgL7C5qqYXpWJJkqQxsmCYqqojwHZgD3AHsKuq9ie5MsnmxS5QkiRpnK3sp1FV7QZ2z9l2xVHant+9LEmSpOODd0CXJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHfYWpJBuT3JlkJsnl8+x/XZLbk9yW5JNJTh98qZIkSeNnwTCVZAWwA9gEbAC2Jtkwp9mtwGRVPR+4Hnj7oAuVJEkaR/2MTJ0LzFTVgap6BLgO2NLboKpurKqH29W9wJrBlilJkjSe+glTq4GDPeuH2m1Hcynwifl2JNmWZDrJ9OzsbP9VSpIkjamBTkBPcjEwCbxjvv1VtbOqJqtqcmJiYpCnliRJGomVfbQ5DKztWV/TbnucJC8G3gi8qKq+PZjyJEmSxls/I1P7gPVJzkhyInARMNXbIMk5wPuBzVV13+DLlCRJGk8LhqmqOgJsB/YAdwC7qmp/kiuTbG6bvQM4CfhYkr9LMnWUw0mSJC0p/Vzmo6p2A7vnbLuiZ/nFA65LkiTpuOAd0CVJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1EFfYSrJxiR3JplJcvk8+5+a5KPt/puTrBt0oZIkSeNowTCVZAWwA9gEbAC2Jtkwp9mlwINVdSbwLuBtgy5UkiRpHPUzMnUuMFNVB6rqEeA6YMucNluAD7bL1wMXJMngypQkSRpP/YSp1cDBnvVD7bZ521TVEeAh4JlzD5RkW5LpJNOzs7NPrmJJkqQxMtQJ6FW1s6omq2pyYmJimKeWJElaFP2EqcPA2p71Ne22edskWQmcAjwwiAIlSZLGWT9hah+wPskZSU4ELgKm5rSZAl7ZLr8M+FRV1eDKlCRJGk8rF2pQVUeSbAf2ACuAq6tqf5IrgemqmgI+AFybZAb4Ck3gkiRJWvIWDFMAVbUb2D1n2xU9y98CfmmwpUmSJI0/74AuSZLUQUY1tSnJLHD3Ip9mFXD/Ip9DT5z9Mn7sk/Fkv4wf+2Q8DaNfTq+qeW9FMLIwNQxJpqtqctR16PHsl/Fjn4wn+2X82CfjadT94mU+SZKkDgxTkiRJHSz1MLVz1AVoXvbL+LFPxpP9Mn7sk/E00n5Z0nOmJEmSFttSH5mSJElaVIYpSZKkDpZEmEqyMcmdSWaSXD7P/qcm+Wi7/+Yk64Zf5fLTR7+8LsntSW5L8skkp4+izuVkoT7paffSJJXEj4Avsn76JMnL2++V/Uk+POwal6M+fn6dluTGJLe2P8MuHEWdy0mSq5Pcl+TzR9mfJO9u++y2JC8YVm3HfZhKsgLYAWwCNgBbk2yY0+xS4MGqOhN4F/C24Va5/PTZL7cCk1X1fOB64O3DrXJ56bNPSHIy8Frg5uFWuPz00ydJ1gNvAF5YVT8O/ObQC11m+vxeeROwq6rOoXke7XuGW+WydA2w8Rj7NwHr23/bgPcOoSZgCYQp4FxgpqoOVNUjwHXAljlttgAfbJevBy5IkiHWuBwt2C9VdWNVPdyu7gXWDLnG5aaf7xWAt9L8wfGtYRa3TPXTJ68GdlTVgwBVdd+Qa1yO+umXAp7RLp8C3DPE+palqvos8JVjNNkCfKgae4FTkzxnGLUthTC1GjjYs36o3TZvm6o6AjwEPHMo1S1f/fRLr0uBTyxqRVqwT9ph8bVV9fFhFraM9fN9chZwVpLPJdmb5Fh/mWsw+umXtwAXJzkE7AZeM5zSdAxP9PfOwKwcxkmkY0lyMTAJvGjUtSxnSZ4CvBO4ZMSl6PFW0ly2OJ9m9PazSX6iqr460qq0Fbimqv4gyc8A1yY5u6q+O+rCNHxLYWTqMLC2Z31Nu23eNklW0gzJPjCU6pavfvqFJC8G3ghsrqpvD6m25WqhPjkZOBv4dJIvAucBU05CX1T9fJ8cAqaq6tGqugv4Ak240uLpp18uBXYBVNVNwNNoHrar0enr985iWAphah+wPskZSU6kmQg4NafNFPDKdvllwKfKu5UutgX7Jck5wPtpgpTzQBbfMfukqh6qqlVVta6q1tHMY9tcVdOjKXdZ6Ofn1w00o1IkWUVz2e/AMItchvrply8BFwAkeR5NmJodapWaawp4RfupvvOAh6rq3mGc+Li/zFdVR5JsB/YAK4Crq2p/kiuB6aqaAj5AMwQ7QzN57aLRVbw89Nkv7wBOAj7Wfh7gS1W1eWRFL3F99omGqM8+2QP8XJLbge8Ar68qR9YXUZ/9chlwVZLfopmMfol/pC+uJB+h+cNiVTtX7c3ACQBV9T6auWsXAjPAw8CrhlabfS9JkvTkLYXLfJLGSJKvJ/mRUdcxDpK8L8m/H/AxL0nyt4M8pqRuDFPSEpLki+2k/qH80k3y6ST/pndbVZ1UVc7pAarq16rqraOuQ9LiMkxJmlf7yddlZTm+ZkndGaakJaj9dNH7gJ9pL7t9td3+1CS/n+RLSf6xvQz19Hbf+UkOJfntJF8G/jjJDyX5iySzSR5sl9e07X8H+Fngj9pz/FG7vZKc2S6fkuRD7dffneRN7f2svjdy1tbzYJK7kmzqeQ2XJDmQ5Gvtvl85ymt9S5Lr0zx/82tJ/nuS/7Fn/3OT/Flbw11JfmOer/2TJP/EPPfY6vM9+3dJ7m9HBn+l52uvSfIf2+VV7fv31SRfSfI3Pe/F89pRvq+mef7e5p5jPDPJVJJ/SvLfgH8xp74fS/JX7THvTPLyhf5/SBosw5S0BFXVHcCvATe1l91ObXf9Hs1H638SOJPm7sBX9Hzp/wD8MHA6zbOtngL8cbt+GvBN4I/ac7wR+Btge3uO7fOU8p9p7uv2IzQ3ZX0Fj/+EzU8Dd9Lcn+ftwAfajzX/IPBuYFNVnQz8z8DfHeMlbwE+1tb+YeCGJCe0YeW/AH/fvtYLgN9M8i/nfO31wKnAn85z7H7es1Xt9lcCO5P86DzHuYzmnlETwLOBfwdUkhPaGv8SeBbNnbT/tOcYO2ge7fMc4H9r/wHQvk9/1b7mZ9E+Iy7zPHNR0uIxTEnLRJLQBKTfqqqvVNXXgN/l8bcK+S7w5qr6dlV9s6oeqKo/q6qH2/a/Q593qk/zsNiLgDdU1deq6ovAHwC/2tPs7qq6qqq+Q/P8zOfQBI3Hajk7ydOr6t6q2n+M091SVddX1aM0d3F/Gs1NR38KmKiqK6vqkXYu11VzXvNNVXVDVX23qr75JN4zgH/fvmefAT4OzDc69Gj7+k5vb8D5N+1H6c+juUXI77U1fgr4C5qH664AXgpcUVXfqKrP88/PGQX4V8AXq+qPq+pIVd0K/BnwS8d4ryQNmPMDpOVjAvgB4Jb883O+Q3MfncfMVtX3HnCc5AeAd9E8qf2H2s0nJ1nRBqBjWUVzD5i7e7bdzeOflfXlxxaq6uG2rpOq6stJfhn432lGqz4HXFZV/3CUcx3sOc5309yD5rk09/957mOXOVsraEbUvu9r59HPe/ZgVX1jzmt87jzHegfN89z+sj3Wzqr6vbbtwTmPIXnsfZqg+Tl9cM6+x5wO/PSc17cSuPYYr0nSgBmmpKVr7k3k7qe5TPfjVXW0RyzM/ZrLgB8FfroNOD8J3EoTKOZrP/d8j9L8wr+93XYafT7eoar2AHva+Un/kWZE6WeP0vx7j5BoL+2tAe4BjgB3VdWxHr+y0GtY6D37oSQ/2BOoTgM+P8/r+RrN+3lZkrOBTyXZ19a5NslTegLVaTSPjZltX8Na4B969j3mIPCZqnrJMV6DpEXmZT5p6fpHYE2ax2HQ/qK+CnhXkmcBJFk9Z/7QXCfThImvJvlhmjsOzz3HvPeUakeudgG/k+TkJKcDrwP+ZKHCkzw7yZZ2TtC3ga/TXPY7mv8pyS+m+TTeb7Zfsxf4b8DX0kyqf3qSFUnOTvJTC9XQvoZ+37P/kOTEJD9Lc+ntY/O8pn+V5Mz20uFDNHcz/y5wM83dmv+Pdp7X+cAvANe17+GfA29J8gPtXKhX9hz2L4Czkvxq+7UnJPmpNB9AkDQkhilp6foUsB/4cpL7222/TfOohb3tp9f+mmbk6Wj+EHg6zQjNXuC/ztn/fwIvS/NpvHfP8/WvAb5B8yy5v6WZKH11H7U/hSZ43UPzCKgXAf/2GO3/H+CXgQdp5mT9Yjsv6Ts04eYngbva1/F/0UyK79dC79mX2/PeQzOB/deOcjlyffu1XwduAt5TVTdW1SM04WlTW997gFf0HGM7zZyqLwPX0HwgAPjeaNfP0czhuqdt8zbgqU/g9UnqyMfJSDquJXkLcGZVXTyCc58P/ElVrRn2uSWND0emJEmSOjBMSZIkdeBlPkmSpA4cmZIkSepgZPeZWrVqVa1bt25Up5ckSerbLbfccn9VTcy3b2Rhat26dUxPT4/q9JIkSX1LcvfR9nmZT5IkqQPDlCRJUgcLhqkkVye5L8n3PWuq3Z8k704yk+S2JC8YfJmSJEnjqZ+RqWtonhh/NJtoHpOwHtgGvLd7WZIkSceHBcNUVX2W5tlYR7MF+FA19gKnJnnOoAqUJEkaZ4OYM7UaONizfqjd9n2SbEsynWR6dnZ2AKeWJEkaraFOQK+qnVU1WVWTExPz3qpBkiTpuDKIMHUYWNuzvqbdJkmStOQNIkxNAa9oP9V3HvBQVd07gONKkiSNvQXvgJ7kI8D5wKokh4A3AycAVNX7gN3AhcAM8DDwqsUqVpIkadwsGKaqausC+wv49YFVJEmSdBzxDuiSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSeqgrzCVZGOSO5PMJLl8nv2nJbkxya1Jbkty4eBLlSRJGj8LhqkkK4AdwCZgA7A1yYY5zd4E7Kqqc4CLgPcMulBJkqRx1M/I1LnATFUdqKpHgOuALXPaFPCMdvkU4J7BlShJkjS++glTq4GDPeuH2m293gJcnOQQsBt4zXwHSrItyXSS6dnZ2SdRriRJ0ngZ1AT0rcA1VbUGuBC4Nsn3HbuqdlbVZFVNTkxMDOjUkiRJo9NPmDoMrO1ZX9Nu63UpsAugqm4CngasGkSBkiRJ46yfMLUPWJ/kjCQn0kwwn5rT5kvABQBJnkcTpryOJ0mSlrwFw1RVHQG2A3uAO2g+tbc/yZVJNrfNLgNeneTvgY8Al1RVLVbRkiRJ42JlP42qajfNxPLebVf0LN8OvHCwpUmSJI0/74AuSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQO+gpTSTYmuTPJTJLLj9Lm5UluT7I/yYcHW6YkSdJ4WrlQgyQrgB3AS4BDwL4kU1V1e0+b9cAbgBdW1YNJnrVYBUuSJI2TfkamzgVmqupAVT0CXAdsmdPm1cCOqnoQoKruG2yZkiRJ46mfMLUaONizfqjd1uss4Kwkn0uyN8nG+Q6UZFuS6STTs7OzT65iSZKkMTKoCegrgfXA+cBW4Kokp85tVFU7q2qyqiYnJiYGdGpJkqTR6SdMHQbW9qyvabf1OgRMVdWjVXUX8AWacCVJkrSk9ROm9gHrk5yR5ETgImBqTpsbaEalSLKK5rLfgQHWKUmSNJYWDFNVdQTYDuwB7gB2VdX+JFcm2dw22wM8kOR24Ebg9VX1wGIVLUmSNC5SVSM58eTkZE1PT4/k3JIkSU9EkluqanK+fd4BXZIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOugrTCXZmOTOJDNJLj9Gu5cmqSSTgytRkiRpfC0YppKsAHYAm4ANwNYkG+ZpdzLwWuDmQRcpSZI0rvoZmToXmKmqA1X1CHAdsGWedm8F3gZ8a4D1SZIkjbV+wtRq4GDP+qF22/ckeQGwtqo+fqwDJdmWZDrJ9Ozs7BMuVpIkadx0noCe5CnAO4HLFmpbVTurarKqJicmJrqeWpIkaeT6CVOHgbU962vabY85GTgb+HSSLwLnAVNOQpckSctBP2FqH7A+yRlJTgQuAqYe21lVD1XVqqpaV1XrgL3A5qqaXpSKJUmSxsiCYaqqjgDbgT3AHcCuqtqf5Mokmxe7QEmSpHG2sp9GVbUb2D1n2xVHaXt+97IkSZKOD94BXZIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHfQVppJsTHJnkpkkl8+z/3VJbk9yW5JPJjl98KVKkiSNnwXDVJIVwA5gE7AB2Jpkw5xmtwKTVfV84Hrg7YMuVJIkaRz1MzJ1LjBTVQeq6hHgOmBLb4OqurGqHm5X9wJrBlumJEnSeOonTK0GDvasH2q3Hc2lwCfm25FkW5LpJNOzs7P9VylJkjSmBjoBPcnFwCTwjvn2V9XOqpqsqsmJiYlBnlqSJGkkVvbR5jCwtmd9TbvtcZK8GHgj8KKq+vZgypMkSRpv/YxM7QPWJzkjyYnARcBUb4Mk5wDvBzZX1X2DL1OSJGk8LRimquoIsB3YA9wB7Kqq/UmuTLK5bfYO4CTgY0n+LsnUUQ4nSZK0pPRzmY+q2g3snrPtip7lFw+4LkmSpOOCd0CXJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHfYWpJBuT3JlkJsnl8+x/apKPtvtvTrJu0IVKkiSNowXDVJIVwA5gE7AB2Jpkw5xmlwIPVtWZwLuAtw26UEmSpHHUz8jUucBMVR2oqkeA64Atc9psAT7YLl8PXJAkgytTkiRpPPUTplYDB3vWD7Xb5m1TVUeAh4Bnzj1Qkm1JppNMz87OPrmKJUmSxshQJ6BX1c6qmqyqyYmJiWGeWpIkaVH0E6YOA2t71te02+Ztk2QlcArwwCAKlCRJGmf9hKl9wPokZyQ5EbgImJrTZgp4Zbv8MuBTVVWDK1OSJGk8rVyoQVUdSbId2AOsAK6uqv1JrgSmq2oK+ABwbZIZ4Cs0gUuSJGnJWzBMAVTVbmD3nG1X9Cx/C/ilwZYmSZI0/rwDuiRJUgcZ1dSmJLPA3Yt8mlXA/Yt8Dj1x9sv4sU/Gk/0yfuyT8TSMfjm9qua9FcHIwtQwJJmuqslR16HHs5l+YUsAABW4SURBVF/Gj30ynuyX8WOfjKdR94uX+SRJkjowTEmSJHWw1MPUzlEXoHnZL+PHPhlP9sv4sU/G00j7ZUnPmZIkSVpsS31kSpIkaVEZpiRJkjpYEmEqycYkdyaZSXL5PPufmuSj7f6bk6wbfpXLTx/98roktye5Lcknk5w+ijqXk4X6pKfdS5NUEj8Cvsj66ZMkL2+/V/Yn+fCwa1yO+vj5dVqSG5Pc2v4Mu3AUdS4nSa5Ocl+Szx9lf5K8u+2z25K8YFi1HfdhKskKYAewCdgAbE2yYU6zS4EHq+pM4F3A24Zb5fLTZ7/cCkxW1fOB64G3D7fK5aXPPiHJycBrgZuHW+Hy00+fJFkPvAF4YVX9OPCbQy90menze+VNwK6qOofmebTvGW6Vy9I1wMZj7N8ErG//bQPeO4SagCUQpoBzgZmqOlBVjwDXAVvmtNkCfLBdvh64IEmGWONytGC/VNWNVfVwu7oXWDPkGpebfr5XAN5K8wfHt4ZZ3DLVT5+8GthRVQ8CVNV9Q65xOeqnXwp4Rrt8CnDPEOtblqrqs8BXjtFkC/ChauwFTk3ynGHUthTC1GrgYM/6oXbbvG2q6gjwEPDMoVS3fPXTL70uBT6xqBVpwT5ph8XXVtXHh1nYMtbP98lZwFlJPpdkb5Jj/WWuweinX94CXJzkELAbeM1wStMxPNHfOwOzchgnkY4lycXAJPCiUdeynCV5CvBO4JIRl6LHW0lz2eJ8mtHbzyb5iar66kir0lbgmqr6gyQ/A1yb5Oyq+u6oC9PwLYWRqcPA2p71Ne22edskWUkzJPvAUKpbvvrpF5K8GHgjsLmqvj2k2parhfrkZOBs4NNJvgicB0w5CX1R9fN9cgiYqqpHq+ou4As04UqLp59+uRTYBVBVNwFPo3nYrkanr987i2EphKl9wPokZyQ5kWYi4NScNlPAK9vllwGfKu9WutgW7Jck5wDvpwlSzgNZfMfsk6p6qKpWVdW6qlpHM49tc1VNj6bcZaGfn1830IxKkWQVzWW/A8Mschnqp1++BFwAkOR5NGFqdqhVaq4p4BXtp/rOAx6qqnuHceLj/jJfVR1Jsh3YA6wArq6q/UmuBKaragr4AM0Q7AzN5LWLRlfx8tBnv7wDOAn4WPt5gC9V1eaRFb3E9dknGqI++2QP8HNJbge+A7y+qhxZX0R99stlwFVJfotmMvol/pG+uJJ8hOYPi1XtXLU3AycAVNX7aOauXQjMAA8Drxpabfa9JEnSk7cULvNJ0vck+ZUkf9mzXknOHGVNkpY2R6YkjVQ72f3ZNJewHnNNVW0f0PELWF9VM4M4niTNddzPmZK0JPxCVf31qIuQpCfDy3ySxlKSS9obVf5RkoeS/EOSC+bsP5Dka0nuSvIrPdv/9ijHPCXJh5LMJrk7yZva+2t97+uS/H6SB9tjbhrOq5V0PHNkStI4+2maR0CtAn4R+PMkZwDfBt4N/FRV3dk+MuKH+zjef6a5z9yP0DwF4S+Be2k+8fvY+T7Ynm8b8IEkq/2UlqRjcWRK0ji4IclXe/69ut1+H/CH7Q0rPwrcCfx8u++7wNlJnl5V91bV/mOdoH147UXAG6rqa1X1ReAPgF/taXZ3VV1VVd+hCVXPoZnPJUlHZZiSNA7+dVWd2vPvqnb74TmjQncDz62qbwC/DPwacG+Sjyf5sQXOsYrmnjR3zzle77O7vvzYQs9DuE96Eq9H0jJimJI0zlanvaNr6zTgHoCq2lNVL6EZPfoH4Kp5vr7X/cCjwOlzjjeUx01IWroMU5LG2bOA30hyQpJfAp4H7E7y7CRbkvwgzfypr9Nc9juq9tLdLuB3kpyc5HTgdcCfLO5LkLTUGaYkjYP/kuTrPf/+73b7zTQP9b0f+B3gZe2jVJ5CE4TuoXlE1IuAf9vHeV4DfIPm2XZ/C3wYuHqgr0TSsuNNOyWNpSSXAP+mqv6XUdciScfiyJQkSVIHhilJkqQOvMwnSZLUgSNTkiRJHYzscTKrVq2qdevWjer0kiRJfbvlllvur6qJ+faNLEytW7eO6enpUZ1ekiSpb0nuPto+L/NJkiR1YJiSJEnqYMEwleTqJPcl+fxR9ifJu5PMJLktyQsGX6YkSdJ46mdk6hpg4zH2b6J53MN6YBvw3u5lSZIkHR8WDFNV9VmaZ18dzRbgQ9XYC5ya5DmDKlCSJGmcDWLO1GrgYM/6oXbb90myLcl0kunZ2dkBnFqSJGm0hjoBvap2VtVkVU1OTMx7qwZJkqTjyiDC1GFgbc/6mnabJEnSkjeIMDUFvKL9VN95wENVde8AjitJkjT2FrwDepKPAOcDq5IcAt4MnABQVe8DdgMXAjPAw8CrFqtYSZKkcbNgmKqqrQvsL+DXB1aRJEnSccQ7oEuSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqYO+wlSSjUnuTDKT5PJ59p+W5MYktya5LcmFgy9VkiRp/CwYppKsAHYAm4ANwNYkG+Y0exOwq6rOAS4C3jPoQiVJksZRPyNT5wIzVXWgqh4BrgO2zGlTwDPa5VOAewZXoiRJ0vjqJ0ytBg72rB9qt/V6C3BxkkPAbuA18x0oybYk00mmZ2dnn0S5kiRJ42VQE9C3AtdU1RrgQuDaJN937KraWVWTVTU5MTExoFNLkiSNTj9h6jCwtmd9Tbut16XALoCqugl4GrBqEAVKkiSNs37C1D5gfZIzkpxIM8F8ak6bLwEXACR5Hk2Y8jqeJEla8hYMU1V1BNgO7AHuoPnU3v4kVybZ3Da7DHh1kr8HPgJcUlW1WEVLkiSNi5X9NKqq3TQTy3u3XdGzfDvwwsGWJkmSNP68A7okSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjroK0wl2ZjkziQzSS4/SpuXJ7k9yf4kHx5smZIkSeNp5UINkqwAdgAvAQ4B+5JMVdXtPW3WA28AXlhVDyZ51mIVLEmSNE76GZk6F5ipqgNV9QhwHbBlTptXAzuq6kGAqrpvsGVKkiSNp37C1GrgYM/6oXZbr7OAs5J8LsneJBvnO1CSbUmmk0zPzs4+uYolSZLGyKAmoK8E1gPnA1uBq5KcOrdRVe2sqsmqmpyYmBjQqSVJkkannzB1GFjbs76m3dbrEDBVVY9W1V3AF2jClSRJ0pLWT5jaB6xPckaSE4GLgKk5bW6gGZUiySqay34HBlinJEnSWFowTFXVEWA7sAe4A9hVVfuTXJlkc9tsD/BAktuBG4HXV9UDi1W0JEnSuEhVjeTEk5OTNT09PZJzS5IkPRFJbqmqyfn2eQd0SZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR10FeYSrIxyZ1JZpJcfox2L01SSSYHV6IkSdL4WjBMJVkB7AA2ARuArUk2zNPuZOC1wM2DLlKSJGlc9TMydS4wU1UHquoR4Dpgyzzt3gq8DfjWAOuTJEkaa/2EqdXAwZ71Q+2270nyAmBtVX38WAdKsi3JdJLp2dnZJ1ysJEnSuOk8AT3JU4B3Apct1LaqdlbVZFVNTkxMdD21JEnSyPUTpg4Da3vW17TbHnMycDbw6SRfBM4DppyELkmSloN+wtQ+YH2SM5KcCFwETD22s6oeqqpVVbWuqtYBe4HNVTW9KBVLkiSNkQXDVFUdAbYDe4A7gF1VtT/JlUk2L3aBkiRJ42xlP42qajewe862K47S9vzuZUmSJB0fvAO6JElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR0YpiRJkjowTEmSJHVgmJIkSerAMCVJktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI66CtMJdmY5M4kM0kun2f/65LcnuS2JJ9McvrgS5UkSRo/C4apJCuAHcAmYAOwNcmGOc1uBSar6vnA9cDbB12oJEnSOOpnZOpcYKaqDlTVI8B1wJbeBlV1Y1U93K7uBdYMtkxJkqTx1E+YWg0c7Fk/1G47mkuBT8y3I8m2JNNJpmdnZ/uvUpIkaUwNdAJ6kouBSeAd8+2vqp1VNVlVkxMTE4M8tSRJ0kis7KPNYWBtz/qadtvjJHkx8EbgRVX17cGUJ0mSNN76GZnaB6xPckaSE4GLgKneBknOAd4PbK6q+wZfpiRJ0nhaMExV1RFgO7AHuAPYVVX7k1yZZHPb7B3AScDHkvxdkqmjHE6SJGlJ6ecyH1W1G9g9Z9sVPcsvHnBdkiRJxwXvgC5JktSBYUqSJKkDw5QkSVIHhilJkqQODFOSJEkdGKYkSZI6MExJkiR1YJiSJEnqwDAlSZLUgWFKkiSpA8OUJElSB4YpSZKkDgxTkiRJHRimJEmSOjBMSZIkdWCYkiRJ6sAwJUmS1IFhSpIkqQPDlCRJUgeGKUmSpA4MU5IkSR30FaaSbExyZ5KZJJfPs/+pST7a7r85ybpBFypJkjSOFgxTSVYAO4BNwAZga5INc5pdCjxYVWcC7wLeNuhCJUmSxlE/I1PnAjNVdaCqHgGuA7bMabMF+GC7fD1wQZIMrkxJkqTx1E+YWg0c7Fk/1G6bt01VHQEeAp4590BJtiWZTjI9Ozv75CqWJEkaI0OdgF5VO6tqsqomJyYmhnlqSZKkRdFPmDoMrO1ZX9Num7dNkpXAKcADgyhQkiRpnPUTpvYB65OckeRE4CJgak6bKeCV7fLLgE9VVQ2uTEmSpPG0cqEGVXUkyXZgD7ACuLqq9ie5EpiuqingA8C1SWaAr9AELkmSpCVvwTAFUFW7gd1ztl3Rs/wt4JcGW5okSdL48w7okiRJHWRUU5uSzAJ3L/JpVgH3L/I59MTZL+PHPhlP9sv4sU/G0zD65fSqmvdWBCMLU8OQZLqqJkddhx7Pfhk/9sl4sl/Gj30ynkbdL17mkyRJ6sAwJUmS1MFSD1M7R12A5mW/jB/7ZDzZL+PHPhlPI+2XJT1nSpIkabEt9ZEpSZKkRWWYkiRJ6mBJhKkkG5PcmWQmyeXz7H9qko+2+29Osm74VS4/ffTL65LcnuS2JJ9Mcvoo6lxOFuqTnnYvTVJJ/Aj4IuunT5K8vP1e2Z/kw8OucTnq4+fXaUluTHJr+zPswlHUuZwkuTrJfUk+f5T9SfLuts9uS/KCYdV23IepJCuAHcAmYAOwNcmGOc0uBR6sqjOBdwFvG26Vy0+f/XIrMFlVzweuB94+3CqXlz77hCQnA68Fbh5uhctPP32SZD3wBuCFVfXjwG8OvdBlps/vlTcBu6rqHJrn0b5nuFUuS9cAG4+xfxOwvv23DXjvEGoClkCYAs4FZqrqQFU9AlwHbJnTZgvwwXb5euCCJBlijcvRgv1SVTdW1cPt6l5gzZBrXG76+V4BeCvNHxzfGmZxy1Q/ffJqYEdVPQhQVfcNucblqJ9+KeAZ7fIpwD1DrG9ZqqrPAl85RpMtwIeqsRc4NclzhlHbUghTq4GDPeuH2m3ztqmqI8BDwDOHUt3y1U+/9LoU+MSiVqQF+6QdFl9bVR8fZmHLWD/fJ2cBZyX5XJK9SY71l7kGo59+eQtwcZJDwG7gNcMpTcfwRH/vDMzKYZxEOpYkFwOTwItGXctyluQpwDuBS0Zcih5vJc1li/NpRm8/m+QnquqrI61KW4FrquoPkvwMcG2Ss6vqu6MuTMO3FEamDgNre9bXtNvmbZNkJc2Q7ANDqW756qdfSPJi4I3A5qr69pBqW64W6pOTgbOBTyf5InAeMOUk9EXVz/fJIWCqqh6tqruAL9CEKy2efvrlUmAXQFXdBDyN5mG7Gp2+fu8shqUQpvYB65OckeREmomAU3PaTAGvbJdfBnyqvFvpYluwX5KcA7yfJkg5D2TxHbNPquqhqlpVVeuqah3NPLbNVTU9mnKXhX5+ft1AMypFklU0l/0ODLPIZaiffvkScAFAkufRhKnZoVapuaaAV7Sf6jsPeKiq7h3GiY/7y3xVdSTJdmAPsAK4uqr2J7kSmK6qKeADNEOwMzST1y4aXcXLQ5/98g7gJOBj7ecBvlRVm0dW9BLXZ59oiPrskz3AzyW5HfgO8PqqcmR9EfXZL5cBVyX5LZrJ6Jf4R/riSvIRmj8sVrVz1d4MnABQVe+jmbt2ITADPAy8ami12feSJElP3lK4zCdJkjQyhilJkqQODFOS/v/27ifEqjKM4/j3p6sBScbAjZgQgYtMp3KnhOAq2lUw+IfaCkEQCFoIzWzdKFqLQNSisE00IIIMpGJRUYuGMV2LyxiYFFFE5Glx35GLjAhzhu7M+P3A4Zz7vOec95y7eu7zct9XktSByZQkSVIHJlOSJEkdmExJGpgkj5JM9W2Hn3H+gSQfLEK/N9ucTZLUmVMjSBqYJHeras0A+r0JbK+qmf+7b0krj5UpSUtOqxwdTXItyR9JXmnxsSQH2/HHSW4kmU7yfYutSzLRYr8n2driLyaZTHI9ySkgfX3tb31MJfkqyeq2nU3yd3uGTwbwNUhaJkymJA3S0BPDfKN9bber6jXgC+D4PNceBl6vqq3AgRYbB/5qsc+Ab1r8c+CXqnoV+BF4CR4vAzIK7KiqEXozjO8DRoANVbWlPcOZRXxnSSvMsl9ORtKydr8lMfM517c/Nk/7NPBdkgl669cB7ATeA6iqS60i9QLwFvBui19IMtvO3w28CfzZljQaAv4BzgMvJzkJXAAmF/6KklY6K1OSlqp6yvGcd4AvgTfoJUML+XEY4OuqGmnb5qoaq6pZYBtwhV7V69QC7i3pOWEyJWmpGu3b/9bfkGQVsLGqLgOHgLX0Fs3+md4wHUl2ATNVdQe4Cuxt8beB4Xarn4D3k6xvbeuSbGr/9FtVVT8AR+glbJI0L4f5JA3SUJKpvs8Xq2pueoThJNPAA2DPE9etBr5NspZedelEVf2bZAw43a67B3zYzh8HziW5DvwK3AKoqhtJjgCTLUF7CHwE3AfOtBjAp4v3ypJWGqdGkLTkOHWBpOXEYT5JkqQOrExJkiR1YGVKkiSpA5MpSZKkDkymJEmSOjCZkiRJ6sBkSpIkqYP/AM4etnM/frMUAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_visualization(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play a round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations: 50 | Total Reward: -50\n"
     ]
    }
   ],
   "source": [
    "play(env, Q_table_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : : : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-01252175b71f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrender_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_table_trained\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-fb68dcc8e543>\u001b[0m in \u001b[0;36mrender_performance\u001b[0;34m(env, Q)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_sleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "render_performance(env, Q_table_trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
