{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning using tables\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome \n",
    "This workshop is structured such that for each cell, you will write your own code and the code will then be asserted in the next cell. The assertion cell is marked \"# Do not edit - Assertion cell #\".\n",
    "\n",
    "The code should be written between \"Write code below\" and \"Write code above\". If your code do not pass the assertion, you will have to rewrite it before continuing.\n",
    "\n",
    "### Task:\n",
    "Click on the cell below, and run it to import the necessary libaries\n",
    "Hot key to run a cell: CTRL + ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_input_by_2(a_variabel):\n",
    "\n",
    "    \"Write code below\" \n",
    "    result = a_variabel * 2\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(multiply_input_by_2(10) == 20), \"Your method did not multiple the input by 2\"\n",
    "print(\"Great, you correctly implemented the method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import neccesary packages\n",
    "To implement the native Q-learning algorithm and use it in a environment, we just need two additional packages.\n",
    "\n",
    "_Numpy_\n",
    "- Numpy adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. We could do this workshop using native Python arrays and built-in methods, however using Numpy simplifies the process. \n",
    "- To call the methods in the package, we use the \"as np\" command. Such that, to create a numpy table with 2 rows and 2 columns where all cells are zero you can simply write np.zeros(2, 2).\n",
    "\n",
    "_OpenAI Gym_\n",
    "- The Gym library is a collection of demo problems an associated environments, that you can use to work out your reinforcement learning algorithms. These range from simple text problems, to complex physisical problems, to Atari video games. OpenAI Gym the is thus the prefeered framework to learn and test Reinforcement learning algorithms.\n",
    "\n",
    "\n",
    "### Task:\n",
    "To import the necessary packages, simply run the cell below, and then run the assertion cell to verify that they have been imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(gym)\n",
    "    print(\"Great, the packages were imported correctly!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem introduction\n",
    "\n",
    "The environment we are going to use is a simple grid world, where the agent is controlling a taxi. The environment was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop the passenger off in another. You can read more about the environment here: https://gym.openai.com/envs/Taxi-v2/\n",
    "\n",
    "### Task:\n",
    "Create the environment variable containing all necessary methods to run the Taxi-v2 game. <br>\n",
    "_Tip: Just run the cell below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_id = \"Taxi-v2\"\n",
    "env = gym.make(environment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(env)\n",
    "    print(\"You successfully created the environement {}\".format(env.spec.id))\n",
    "except:\n",
    "    print(\"You create the environment incorrectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in additon to support functions that will be used throughout the workshop. \n",
    "\n",
    "### Task\n",
    "Do as you have done before, simply mark the cell below and press CTRL + Enter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TestEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.reset()\n",
    "        \n",
    "    def play(self, action):\n",
    "        if not (0 <= int(action) <= 5):\n",
    "            print(\"Action value must either be 0,1,2,3,4 or 5\")\n",
    "            return\n",
    "\n",
    "        clear_output()\n",
    "        _, reward, done, _ = self.env.step(action)\n",
    "        self.env.render()\n",
    "        print(\"Reward: \", reward)\n",
    "        \n",
    "        if(done):\n",
    "            print(\"Game completed, resetting environment!\")\n",
    "            self.reset_env()\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.env.reset()\n",
    "        print(\"Environment has been reset\")\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "\n",
    "class MockData():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.seed(10)\n",
    "        \n",
    "    def get_Q(self):\n",
    "        Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q[0:2,0] = 10\n",
    "        return Q\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "\n",
    "def visualize_q_table():\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "    heat_map = sb.heatmap(Q_trained)\n",
    "    plt.show()\n",
    "\n",
    "def plot_visualization(data):\n",
    "    \n",
    "    reward_list = data[0]\n",
    "    iteration_list = data[1]\n",
    "    epsilon_list = data[2]\n",
    "    \n",
    "    episodes = range(len(reward_list))\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(311)\n",
    "    ax.set_title(\"Reward\")\n",
    "    ax = plt.plot(episodes, reward_list)\n",
    "\n",
    "    ax = plt.subplot(312)\n",
    "    ax.set_title(\"Iterations per episode\")\n",
    "    ax.plot(episodes, iteration_list)\n",
    "    \n",
    "    ax = plt.subplot(313)\n",
    "    ax.set_title(\"Epsilon\")\n",
    "    ax.set_xlabel(\"Episodes\")\n",
    "    ax.plot(episodes, epsilon_list)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def reset_env_and_update_params(env):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    return state, done, iterations, total_reward\n",
    "\n",
    "def render_performance(env, Q):\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    t_sleep = 1.2\n",
    "    \n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    time.sleep(t_sleep)\n",
    "    clear_output()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state,:]) \n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        state =  new_state\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(t_sleep)\n",
    "        clear_output()\n",
    "        \n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        if (iterations >= 20): \n",
    "            done = True\n",
    "            print(\"Agent did not complete the episode within 20 iterations, train your agent better!\")\n",
    "            return\n",
    "    \n",
    "    print(\"Your agent completed the task using {} iterations, \\\n",
    "          and got a total reward of {}\".format(iterations, total_reward))\n",
    "    \n",
    "test_env = TestEnvironment()\n",
    "current_path = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "mock = MockData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(mock)\n",
    "    print(\"Great, the support methods were created!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with environment\n",
    "Before we start creating the Q-learning agent, we will explore the environment we are going to use. This is done using the support methods we have created above.\n",
    "\n",
    "**How to play**\n",
    "1. Simply input an action, e.g. the integer 1, where it is indicated in the cell below, and click CTRL + Enter. \n",
    "2. The game will reset when you have picked up and then dropped off the passenger at the indicated location.\n",
    "3. If you would like to reset the test environment during the execution, run the cell which indicates this below. \n",
    "\n",
    "*Tip*: The bold colored letter shows where the passenger should be picked up, and the normal colored letter shows drop of spot.\n",
    "\n",
    "**Possible actions**\n",
    "0. Move down\n",
    "1. Move up\n",
    "2. Move right\n",
    "3. Move left\n",
    "4. Pick up passenger\n",
    "5. Drop off passenger\n",
    "\n",
    "### Task:\n",
    "Play around with the environment to understand the rewards and how the game dynamics works. When you are confident, you understand it move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to reset the environment\n",
    "test_env.reset_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to play the game\n",
    "\n",
    "# Input your action below\n",
    "action = 3\n",
    "# Input your action above\n",
    "\n",
    "test_env.play(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion point\n",
    "The game above is a simple environment, which is easy for humans to solve. However, if you were to use traditional programming, how would you solve it? \n",
    "\n",
    "### Task\n",
    "Use 5 minutes in the group to come up with a strategy, and create a quick draft!\n",
    "\n",
    "<img src=\"images/Discussion.jpg\" alt=\"drawing\" width=\"400\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "In Reinforcement learning an agent moves from a state $S_{t}$, to a new state $S_{t+1}$ by taking an action $A_{t}$, and recieving the reward $R_{t}$ as illustrated below. The agent will repeat this process for a defined amount of iterations, and this process as a whole is known as an episode. The goal of the Reinforcement learning agent is to maximize the total reward it is able to collect during an episode. To improve the performance, it learns from each state transition, i.e. move from $S_{t}$ to $S_{t+1}$. How it learns, is what seperates the different Reinforcement learning algorithms. \n",
    "\n",
    "<img src=\"images/Agent.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "In this notebook we will work with one of the most iconic Reinforcement learning algorithms in its simplest form, namely Q-learning using Q-tables. The Q-tables holds the values of the strategy that givesus the highest reward,  according to the agents knowledge. The table can be thought of as the brain of the agent. Each row in the table represents a state, and the columns represent the possible actions in that state. This will be explained further in the next section.\n",
    "\n",
    "A Q-value is a measure of the total expected reward the agent will recieve if it chooses that action, and always picks the action with the highest Q-value in the succeeding states, based on its current knowledge of the environment, i.e. the Q-table. Simply said, the higher the Q-value the better the agent believes the action is. \n",
    "\n",
    "The Q-learning algorithm is used to update the Q-values for a given action $A_{t}$ in the state $S_{t}$, and is updated after each iteration. The equation is shown below.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "A key point to note in this equation, is that we are updating the Q(s,a) by using the the highest Q-value in the next state, $S_{t+1}$. This value is unknown, and it is therefore just the agent's estimate of that state. This makes it an optimization problem were we gradually adjust each Q-value as we recieve a reward moving between the states.\n",
    "\n",
    "\n",
    "This might seem daunting at first, but as we through this notebook break it into smaller pieces it will hopefully be easier to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Q-table\n",
    "The Q-table is the brain of agent, and stores the agent's current knowledge of the Q-values of each state. Each row represents a state, and the columns represent all possible actions in that state. As a result, the table's dimensions will have the number of rows=number of states, and the number of columns=number of actions. An illustration of the table is shown below, albeit without the labels on the columns. \n",
    "\n",
    "<img src=\"images/Taxi_matrix_initial.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "### Task:\n",
    "Create and return a Q-table using numpy, where each cell is initialized to zeros:\n",
    "- Create the Q-table by using the numpy command: np.zeros(rows, columns). <br>\n",
    "    https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\n",
    "- Use the following command to get the number of states in the environment:\n",
    "    - env.observation_space.n\n",
    "- Use the following command to get the number of possible actions. PS. all states have the same amount of actions.\n",
    "    - env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_table(env):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    action_size = env.action_space.n\n",
    "    observation_size = env.observation_space.n\n",
    "    \n",
    "    Q_table = np.zeros([observation_size, action_size])\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(np.count_nonzero(mock.get_Q() == 0)), \"All values in Q-table should be zero\"\n",
    "assert(mock.get_Q().shape == (mock.get_env().observation_space.n, mock.get_env().action_space.n)), \\\n",
    "\"The dimensions are wrong\"\n",
    "print(\"The Q-table was correctly built! \" +\n",
    "      \"It has {} rows, each representing a unique state, and {} columns, each representing an action for that state.\"\\\n",
    "      .format(mock.get_env().observation_space.n, mock.get_env().action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best action\n",
    "Before we implement the Q-learning algorithm, we will implement a method that, given a state, chooes the action with the highest Q-value.\n",
    "\n",
    "### Task\n",
    "Return the column index of the highest Q-value in the Q-table, given a state.\n",
    "- Tip: Use the argmax function from the numpy library. <br> https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(Q_table, state):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    best_action =  np.argmax(Q_table[state,:]) \n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(get_best_action(mock.get_Q(), 1) == 0), \"The method did not pick the action with the highest Q_value\"\n",
    "print(\"The best action for the test state was chosen, excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an action\n",
    "\n",
    "Initially all the Q-values in the Q-table are set to zero. As we begin to move around the environment we can end up ina loop, such that we only explore a small part of the environment. This problem is so common its gotten a name; The Exploration vs Exploiation dilemma!\n",
    "\n",
    "\n",
    "# TODO\n",
    "One simply and highly solution is simply, and is used through \n",
    "The exploration vs. exploitation is a very effective method to ensure the agent explores a sufficent area of the state space, and avoid coverging to a local optima.\n",
    "\n",
    "### Task:\n",
    "Create a method that randomly picks an action IF a random value in the range [0,1] is less than epsilon, or ELSE picks the best action using the method we created earlier.   \n",
    "- To pick a random action, use env.action_space.sample()\n",
    "- To generate a random number with uniform probability in the range [0,1], use np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q_table, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    if epsilon > np.random.uniform():\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = get_best_action(Q_table, state)\n",
    "    \n",
    "    \"Write code above\"    \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(select_action(mock.get_Q(), 1, 0) == 0), \\\n",
    "\"Method should always return the same value for this state, since epsilon is 0\"\n",
    "assert not (len(set([select_action(mock.get_Q(), 1, 1) for x in range(20)])) <= 2), \\\n",
    "\"Method should not return identical values when a random action, should be chosen\"\n",
    "print(\"The correct actions were picked, great job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradually shift towards exploitation, and reduce exploration of state space\n",
    "To make sure we are gradually move from exploration to exploitation, we need to reduce the possibility of choosing a random action. In other words, we need to reduce epsilon. This is done by multiplying it with a constant called epsilon decay.\n",
    "\n",
    "### Task:\n",
    "Create a method which reduces epsilon by multiplying itself with the defined constant epsilon decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon(epsilon):\n",
    "    epsilon_decay = 0.95\n",
    "    \n",
    "    \"Write code below\" \n",
    "    epsilon *= epsilon_decay\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(update_epsilon(5) == 4.75), \"Given an input of 5, the output should have been 4.75\"\n",
    "print(\"Epsilon was correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Find the highest Q_value for a given state\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ {\\color{red}{\\underset{a}{max} \\  Q(s_{t+1},a)}} - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}\n",
    "$ \n",
    "\n",
    "### Task\n",
    "Here we want the implemnt the equation highlightet in red above, this is simply done by choosing the highest Q-value for a given state. In other words the Q-value of the best action.\n",
    "\n",
    "Tip: Use the numpy.max function to retrive the coulumn (action) with the highest Q-value for the given row(state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_Q_value_in_state(Q_table, new_state):\n",
    "    \"Write code below\"\n",
    "    best_Q_value = np.max(Q_table[new_state,:])\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(find_highest_Q_value_in_state(mock.get_Q(), 1) == 10), \\\n",
    "\"The method did not pick the action with the highest Q_value\"\n",
    "print(\"Perfect, the method returned the highest Q-value for that state!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate error  in Q-value\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[{\\color{red}{r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})}}\\big]\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "In this section we want to calculate the error in the Q-value, i.e. the difference in the Q-value from the previous iteration and the one calculated now. \n",
    "\n",
    "In the equation above you can see the \\gamma, this is called the discount factor. We use this to determine the importance of future rewards. If this value is close to zero it will make the agent short-sighted by focusing on immidate rewards. If we bring the value close to one it will make the agent value the long time reward. \n",
    "\n",
    "\n",
    "### Task:\n",
    "Implement a method that calculates the error of the q value. You shouldn't need to use any external libraries here, implementing the equation above with the discount_factor given with native python code should be sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_in_Q_value(Q_table, state, action, reward, new_state):\n",
    "    \n",
    "    discount_factor = 0.95\n",
    "    \n",
    "    \"Write code below\"\n",
    "    highest_q_value_in_state = find_highest_Q_value_in_state(Q_table, new_state)\n",
    "    \n",
    "    error_in_Q_value = reward + discount_factor * highest_q_value_in_state - Q_table[state,action]\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return error_in_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(calculate_error_in_Q_value(mock.get_Q(), 0, 0, 10, 1) == 9.5), \\\n",
    "\"The method did not calculate the correct error value for the test sample\"\n",
    "print(\"The error in Q-value have been calculated correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Q-table\n",
    "\n",
    "Finally, we can put it all together and end up with the final equation.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow {\\color{red}{Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "### Task\n",
    "Create a method which either sets\n",
    " - The Q-value equal to the reward, if it was the final episode. (Done is true)\n",
    " - Else updates the Q-value by following the equation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(Q_table, state, action, done, reward, new_state):\n",
    "    \n",
    "    learning_rate = 0.8\n",
    "    \n",
    "    \"Write code below\"\n",
    "    if(done):\n",
    "        Q_table[state, action] = reward\n",
    "    else:\n",
    "        Q_table[state, action] = Q_table[state, action] + \\\n",
    "        learning_rate * calculate_error_in_Q_value(Q_table, state, action, reward, new_state)\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(update_q_table(mock.get_Q(), 2, 1, 1, 10, 3)[2,1] == 10), \\\n",
    "\"The updated Q-value when simulation was done was incorrect\"\n",
    "assert(update_q_table(mock.get_Q(), 0, 0, 0, 10, 1)[0,0] == 17.6), \\\n",
    "\"The updated Q-value when the episode was not done, was incorrect\"\n",
    "print(\"Q-tabel have been correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take one step\n",
    "We are now ready to use all the methods we've implemented in order to tell the agent to do an action and push that into the environment.\n",
    "\n",
    "### Task\n",
    "    Select action\n",
    "    Tell the environment to do the chosen action\n",
    "    Return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(Q_table, env, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    action = select_action(Q_table, state, epsilon)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return action, reward, done, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "mock_env1 = deepcopy(mock.get_env())\n",
    "mock_env1.reset()\n",
    "mock_env2 = deepcopy(mock_env1)\n",
    "\n",
    "new_state, _, _, _ = mock_env1.step(0)\n",
    "assert(step(mock.get_Q(), mock_env2, 0, 0) == (0, -1, False, new_state)), \\\n",
    "\"Your environment did not return the correct values\"\n",
    "print(\"Your step method seems to function properly, good job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train method\n",
    "\n",
    "# TODO\n",
    "Explain what an episode and iteration is\n",
    "Explain code already writen\n",
    "\n",
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, num_episodes, max_iterations):\n",
    "    done = False\n",
    "    epsilon = 1\n",
    "    reward_list, iterations_list, epsilon_list = [], [], []\n",
    "    \n",
    "    \"Write code below\"\n",
    "    # Initialize Q-table\n",
    "    Q_table = create_q_table(env)\n",
    "    \"Write code above\"\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        if (done and episode % 10 == 0): \n",
    "            print(\"Episode: {} | Iterations: {} | Total Reward: {}\".format(episode, iterations, total_reward))\n",
    "        \n",
    "        state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "        \n",
    "        \"Write code below\"\n",
    "        # Update epsilon\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "        \n",
    "        \"Write code above\"\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            \"Write code below\"\n",
    "            \n",
    "            # Take step\n",
    "            action, reward, done, new_state = step(Q_table, env, state, epsilon)\n",
    "                        \n",
    "            # Update Q-table\n",
    "            Q_table = update_q_table(Q_table, state, action, done, reward, new_state)\n",
    "            \n",
    "            # Set the new state as the current state\n",
    "            state = new_state\n",
    "            \n",
    "            \"Write code above\"\n",
    "            \n",
    "            # Update episode information\n",
    "            iterations += 1\n",
    "            total_reward += reward\n",
    "            \n",
    "            # End episode if it has lasted for too many iterations\n",
    "            if (iterations >= max_iterations): done = True\n",
    "\n",
    "        reward_list.append(total_reward)\n",
    "        iterations_list.append(iterations)\n",
    "        epsilon_list.append(epsilon)\n",
    "        \n",
    "    return Q_table, (reward_list, iterations_list, epsilon_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create play method\n",
    "\n",
    "\n",
    "### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, Q_table):\n",
    "    \n",
    "    state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        \"Write code below\"\n",
    "        # Take step\n",
    "        action, reward, done, new_state = step(Q_table, env, state, 0)\n",
    "\n",
    "        # Set the new state as the current state\n",
    "        state = new_state\n",
    "\n",
    "        \"Write code above\"\n",
    "\n",
    "        # Update episode information\n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        if (iterations >= 50): done = True\n",
    "    \n",
    "    print(\"Iterations: {} | Total Reward: {}\".format(iterations, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time to put our methods to use!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_episodes = 1000\n",
    "maximum_iterations = 50\n",
    "\n",
    "Q_table_trained, data = train(env, maximum_episodes, maximum_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot key variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_visualization(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play a round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, Q_table_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_performance(env, Q_table_trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
