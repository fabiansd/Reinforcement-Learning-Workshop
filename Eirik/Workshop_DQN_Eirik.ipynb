{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement learning has resulted superman performance in a range of scenarios, such as Chess, Atari 2600, and Starcraft, but also in robotics. The main reason for this success compared to traditional Reinforcement Learning, is the introduction of deep neural networks. The first successful implementation of deep neural networks in combination with a Reinforcement learning algorithm was Mnih et. al. which was able to reach superhuman performance in the classic Atari games. Their approach was coined Deep Q-Networks(DQN), which is very similar to the Q-learning algorithm we implemented in the previous workshop, in combination with deep neural networks. This is the algorithm we will be implementing in this notebook.\n",
    "\n",
    "In the DQN approach, a neural network replaces the Q-table which we used in the previous notebook. The main reason for this is the neural networks ability to generalize around similar states. In a continuous environment a Q-table would quickly grow to tens of millions of unique states, although many of these are so similar that the same action should be applied. This is both an issue due to the huge table we would need, but also because we would need a tremendous amount of training data to update all the states. \n",
    "\n",
    "Neural networks address this issue, however they do come with their own challenges. Neural network struggles with instabilities during training, in addition to critical forgetting and divergence. Mnih's success was due to several ingenious tricks which stabilized the network during training, and we will go through these during the workshop.\n",
    "\n",
    "In the previous notebook we created a lot of methods, which resulted in a lot of parameters being passed from method to method. This can quickly become convoluted, and in this notebook we instead use classes. An example can be seen below. When the class is created the init method is ran, and the different parameters are set and stored in this instance of the class. The parameters can be called within the class using \"self.<name_of_parameter>\" syntax, e.g. self.learning_rate."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Q-Networks\n",
    "There are several frameworks which can be used to create neural networks such as Tensorflow from Google, PyTorch from Facebook or MXNet from Apache. In this workshop we will be using Tensorflow, with a high-level API called Keras on top. Keras greatly simplifies the effort needed to create a neural network, and it is often referred to as the best Deep Learning framework for those who are just starting out.\n",
    "\n",
    "Neural networks are partially inspired by the structure of the human brain and consist of a network of interconnected neurons. The neural network consists of an input layer, a set of hidden layers, and an output layer, as shown in the figure below.\n",
    "\n",
    "<img src=\"images/Neural_network__achitecture.svg\" alt=\"drawing\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "\n",
    "\n",
    "#### Task\n",
    "Now we will see how easy it is to create a neural network using Keras. We are going to create a neural network with an input layer, two hidden layers, and one output layer. The design criterions are as follows: <br>\n",
    "\n",
    "1. The model should be fully connected \n",
    "<br>\n",
    "<br>\n",
    "2. The input layer and first hidden layer is created together, and should:\n",
    "    - Be of type Dense\n",
    "    - Use the ReLU activation function\n",
    "    - Have input size equal to the observations size of the environment\n",
    "    - Have hidden layers size equal to parameter hidden_layer_1 \n",
    "<br>\n",
    "<br>\n",
    "3. The second hidden layer should:\n",
    "    - Be of type Dense\n",
    "    - Use the ReLU activation function \n",
    "    - The hidden layer should have size equal to parameter hidden_layer_2\n",
    "    - Tip1: This layer is similar to previous, but without the input_dim parameter\n",
    "    - Tip2: Remember to add the dense layer using model.add()\n",
    "<br>  \n",
    "<br>\n",
    "4. The output layer should:\n",
    "    - Be of type Dense\n",
    "    - The activation function should be linear\n",
    "    - Should have size equal to action vector, i.e. action_size\n",
    "<br>\n",
    "<br>\n",
    "5. The final part is to compile the network. Before we do this we must define som parameters\n",
    "    - Loss metric should be Mean-squared-error (MSE)\n",
    "    - Optimizer should be Adaptive Moment Estimation (Adam)\n",
    "    - Learning rate should equal to learning_rate\n",
    "    - Learning rate decay should be equal to learning_rate_decay\n",
    "<br>\n",
    "As you can see below step 1,2 and 5 is already done, so your task is to complete task 3 and 4.\n",
    "\n",
    "**Ps!** <br>\n",
    "At the end of the workshop you can experiment with different structures and parameters, however in the first walkthrough, you will follow the defined steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "should_assert = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.metrics import MSE\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "class QNetwork:\n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.observations_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.learning_rate_decay = parameters[\"learning_rate_decay\"]\n",
    "        self.loss_metric = parameters[\"loss_metric\"]\n",
    "        self.hidden_layer_1 = parameters[\"hidden_layer_1\"]\n",
    "        self.hidden_layer_2 = parameters[\"hidden_layer_2\"]\n",
    "        \n",
    "    def build_q_network(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_layer_1, input_dim=self.observations_size, activation='relu'))\n",
    "        \n",
    "        \"Input code below\"\n",
    "        model.add(Dense(self.hidden_layer_2, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        \n",
    "        \"Input code above\"\n",
    "        \n",
    "        model.compile(loss=self.loss_metric, optimizer=Adam(lr=self.learning_rate, decay=self.learning_rate_decay))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(should_assert):\n",
    "    parameters = {\"loss_metric\" : \"mse\",\n",
    "                \"learning_rate\" : 0.01,\n",
    "                \"learning_rate_decay\": 0.01,\n",
    "                \"hidden_layer_1\": 24,\n",
    "                \"hidden_layer_2\": 24}\n",
    "\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    q_network = QNetwork(env, parameters)\n",
    "    model = q_network.build_q_network()\n",
    "    config_network = model.get_config()\n",
    "    assert(config_network.get(\"name\")[0:10] == \"sequential\")\n",
    "\n",
    "    # Layers\n",
    "    config_layers = config_network.get(\"layers\")\n",
    "    assert(len(config_layers) == 3), \"You should only have 2 dense layers\"\n",
    "\n",
    "    # First layer\n",
    "    assert(config_layers[0].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[0].get(\"config\").get(\"batch_input_shape\") == (None, q_network.observations_size))\n",
    "    assert(config_layers[0].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[0].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
    "    \"Activation function for first layer should be relu\"\n",
    "\n",
    "    # Second layer\n",
    "    assert(config_layers[1].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[1].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[1].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
    "    \"Activation function for second layer should be relu\"\n",
    "\n",
    "    # Thrid layer\n",
    "    assert(config_layers[2].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[2].get(\"config\").get(\"units\") == q_network.action_size), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[2].get(\"config\").get(\"activation\") == \"linear\"),\\\n",
    "    \"Activation function for third layer should be linear\"\n",
    "\n",
    "\n",
    "    config_optimizer = model.optimizer.get_config()\n",
    "    assert(0.0099 < config_optimizer.get(\"lr\") <= 0.01),\"Learning rate should be 0.01\"\n",
    "    assert(0.0099 < config_optimizer.get(\"decay\") <= 0.01),\"Learning rate decay should be 0.01\"\n",
    "    assert(model.loss == \"mse\"), \"Loss metric should me mse\"\n",
    "    \n",
    "    print(\"Superb, you implemented the layers correct. PS, Ignore the warning above!\")\n",
    "    print(\"Information about your model is listed below. PS. The input layer is not shown,\")\n",
    "    print(\"altough you should be able to calculate it by looking at <Param #> of the first hidden layer.\")\n",
    "    model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Experience Buffer\n",
    "\n",
    "For a neural network to perform optimally we want the data to be I.I.D (Independent and Identically Distributed). In supervised learning, where for instance the network is fed an image and it will predict either cat or dog, this is achived by randomly sampling the training data from the full data set. As a result:\n",
    "- Batches have close-to similar data distribution.\n",
    "- Samples in each batch are independent of each other.\n",
    "\n",
    "In Reinforcement learning where our agent samples data by moving from state to state, the recently sampled data will be highly correlated to each other. As a result, we will feed our network data which closely resembles each other, this a recipe for disaster when working with neural network, as it leads to overfitting. Overfitting \n",
    "\n",
    "In addition, the data distribution of the initial states will be different from that of the later stage, i.e. this does not satifiy the I.I.D criterion. This is bad news!\n",
    "\n",
    "Luckly Mnih et. al. has a solution. Instead of training on the just the previously collected samples we store all states in a **Experience Buffer** and sample randomly from this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "class ExperienceReplay:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.buffer_size = parameters[\"buffer_size\"]\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        self.experience_buffer = deque(maxlen=self.buffer_size)\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"])\n",
    "    \n",
    "    def add_experience(self, state, action, reward, done, new_state):   \n",
    "        e = self.experience(state, action, reward, done, new_state)\n",
    "        self.experience_buffer.append(e)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        if len(self.experience_buffer) < self.batch_size:\n",
    "            experiences =  self.experience_buffer\n",
    "        else:\n",
    "            experiences = random.sample(self.experience_buffer, self.batch_size)\n",
    "        \n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
    "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
    "        new_states = np.vstack([e.new_state for e in experiences if e is not None])\n",
    "        dones = np.vstack([e.done for e in experiences if e is not None])\n",
    "        \n",
    "        return (states, actions, rewards, dones, new_states)\n",
    "    \n",
    "    def warm_up(self, env):\n",
    "        for _ in range(10):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                self.add_experience(state, action, reward, done, new_state)\n",
    "                state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent\n",
    "\n",
    "### Target network\n",
    "In addition, the labels are stationary, e.g. an image of a cat will always have the label cat. This is not the case in Reinforcement learning.\n",
    "\n",
    "As we experienced in the previous notebook, the Q-value of the next states is calculated based on the assumption that the Q-table is correct. However, since we begin each training session with a blank table and update the values as we go along, the highest Q-value which is used in the Q-learning equation\n",
    "\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import gym\n",
    "\n",
    "class Agent:    \n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.env = env\n",
    "        self.local_network = QNetwork(env, parameters).build_q_network()\n",
    "        self.target_network = QNetwork(env, parameters).build_q_network()\n",
    "        self.experience_replay = ExperienceReplay(parameters)\n",
    "        \n",
    "        self.epsilon = parameters[\"epsilon_init\"]\n",
    "        self.epsilon_decay = parameters[\"epsilon_decay\"]\n",
    "        self.epsilon_minimum = parameters[\"epsilon_minimum\"]\n",
    "        self.tau = parameters[\"tau\"]\n",
    "        self.gamma = parameters[\"gamma\"]\n",
    "        self.epochs = parameters[\"epochs\"]\n",
    "    \n",
    "    def learn(self):\n",
    "        states, actions, rewards, dones, next_states = self.experience_replay.get_batch()\n",
    "        \n",
    "        # Get Q-values for next state\n",
    "        Q_target = self.target_network.predict(next_states)\n",
    "\n",
    "        # Apply Q-learning algorithm to calculate the actual Q-value for state\n",
    "        Q_calc = rewards + (self.gamma * np.amax(Q_target, axis=1).reshape(-1, 1) * (1 - dones))\n",
    "        \n",
    "        # Calculate the predicted Q-value for the action taken in the state using local network\n",
    "        Q_local = self.local_network.predict(states)\n",
    "        \n",
    "        # Change Q_values for chosen action with \"correct\" Q-values, e.g. Q_actual        \n",
    "        for row, col_id in enumerate(actions):\n",
    "            Q_local[row, np.asscalar(col_id)] = Q_calc[row]\n",
    "        \n",
    "        # Network inputs states and outputs \n",
    "        self.local_network.fit(states, Q_local, epochs=self.epochs, verbose=0)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        local_weights = self.local_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "\n",
    "        for i in range(len(local_weights)):\n",
    "            target_weights[i] = self.tau * local_weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        self.target_network.set_weights(target_weights)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon >= self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def select_action(self, state):\n",
    "    \n",
    "        if self.epsilon > np.random.uniform():\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.local_network.predict(np.array([state])))\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def step(self, env, state):\n",
    "        \n",
    "        action  = self.select_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        return action, reward, done, new_state\n",
    "    \n",
    "    def save(self):\n",
    "        save_dir = os.path.join(os.getcwd(), env.spec.id +\"_\"+ datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "        os.makedirs(save_dir)\n",
    "        self.target_network.save(\"target_network.h5\")\n",
    "        self.local_network.save(\"local_network.h5\")\n",
    "        print(\"Weights saved successfully\")\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.target_network.load_weights(path + \"/target_network.h5\")\n",
    "        self.local_network.load_weights(path + \"/local_network.h5\")\n",
    "        print(\"Weights loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, iterations, episodes):\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_reward_list, iterations_list = [], []\n",
    "    agent.experience_replay.warm_up(env)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward=0\n",
    "        \n",
    "        \n",
    "        if (episode != 0): \n",
    "            agent.update_epsilon()\n",
    "    \n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            action, reward, done, new_state = agent.step(env, state)\n",
    "            agent.experience_replay.add_experience(state, action, reward, done, new_state)\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            agent.learn()\n",
    "            agent.update_target_network()\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "        \n",
    "        total_reward_list.append(total_reward)\n",
    "        iterations_list.append(iteration+1)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(\"Episode: {} | Average iterations: {} | Average total reward: {} | Epsilon: {} \" \\\n",
    "                  .format(episode, mean(iterations_list), mean(total_reward_list), agent.epsilon))\n",
    "            total_reward_list.clear()\n",
    "            iterations_list.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agent, env):\n",
    "    \n",
    "    done = False\n",
    "    agent.epsilon = 0\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action, reward, done, new_state = agent.step(env, state)\n",
    "        state = new_state\n",
    "        \n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(env):\n",
    "    \n",
    "    env_id = env.spec.id\n",
    "\n",
    "    if env_id == \"CartPole-v0\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"CartPole-v0\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.95,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.97,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 10000,\n",
    "            \"batch_size\" : 32,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "        \n",
    "    elif env_id == \"CartPole-v1\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"CartPole-v1\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.999,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "        \n",
    "    elif env_id == \"LunarLander-v2\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"LunarLander-v2\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.999,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2500,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "    else:\n",
    "        print(\"Standard hyperparameters {} chosen!\".format(env.spec.id))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.95,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.97,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 10000,\n",
    "            \"batch_size\" : 32,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "environment = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(environment)\n",
    "\n",
    "episodes = 3000\n",
    "iterations = 500\n",
    "parameters = get_hyperparameters(env)\n",
    "\n",
    "dqn_agent = Agent(env, parameters)\n",
    "train(dqn_agent, env, iterations, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run - Record - Show : One simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "monitor = gym.wrappers.Monitor(env, directory=\"videos\", force=True)\n",
    "\n",
    "#Why is three simulations needed for saving?!\n",
    "for _ in range(3):\n",
    "    play(dqn_agent, monitor)\n",
    "\n",
    "monitor.close()\n",
    "env.close()\n",
    "\n",
    "#HTML(\"\"\"\n",
    "#<video width=\"640\" height=\"480\" controls>\n",
    "#  <source src=\"{}\" type=\"video/mp4\">\n",
    "#</video>\n",
    "#\"\"\".format(\"./videos/\"+list(filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we want to plot\n",
    "from matplotlib import pyplot <br>\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "reward_list, episode_list, iteration_list = [], [], []\n",
    "\n",
    "episode_list.append(episode)\n",
    "reward_list.append(total_reward)\n",
    "\n",
    "if episode % 15 == 0:\n",
    "    ax.clear()\n",
    "    ax.plot(episode_list, reward_list)\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
