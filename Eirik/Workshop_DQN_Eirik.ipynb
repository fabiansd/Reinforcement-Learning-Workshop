{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement learning has resulted superman performance in a range of scenarios, such as Chess, Atari 2600, Starcraft and Go. The main difference from traditional Reinforcement Learning is the introduction of deep neural networks.\n",
    "\n",
    "In deep Reinforcement learning neural networks replace the Q-table which was introduced in the previous notebook. The Q-tables quickly becomes infeasable when the space becomes large, and espacally when it is continous. \n",
    "\n",
    "However, using neural networks to approximate the Q-values is not a straight forward task. On of the main issues of earlt attempts using neural networks was the instabilty during training. \n",
    "\n",
    "One of the first algorithms to succesfully solve the instability problem was Deepmind, which introduced the DQN-algorithm (Deep Q-networks algorithm). This is the algorithm which will be introduced in this\n",
    "\n",
    "\n",
    "\n",
    "In the previous notebook we divided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to game we are going to play, e.g. visit website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Q-Networks\n",
    "The main \n",
    "\n",
    "#### Task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import MSE\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class QNetwork:\n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.observations_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.learning_rate_decay = parameters[\"learning_rate_decay\"]\n",
    "        self.loss_metric = parameters[\"loss_metric\"]\n",
    "        self.hidden_layer_1 = parameters[\"hidden_layer_1\"]\n",
    "        self.hidden_layer_2 = parameters[\"hidden_layer_2\"]\n",
    "        \n",
    "    def build_q_network(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_layer_1, input_dim=self.observations_size, activation='relu'))\n",
    "        model.add(Dense(self.hidden_layer_2, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self.loss_metric, optimizer=Adam(lr=self.learning_rate, decay=self.learning_rate_decay))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "class ExperienceReplay:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.buffer_size = parameters[\"buffer_size\"]\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        self.experience_buffer = deque(maxlen=self.buffer_size)\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"])\n",
    "    \n",
    "    def add_experience(self, state, action, reward, done, new_state):   \n",
    "        e = self.experience(state, action, reward, done, new_state)\n",
    "        self.experience_buffer.append(e)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        if len(self.experience_buffer) < self.batch_size:\n",
    "            experiences =  self.experience_buffer\n",
    "        else:\n",
    "            experiences = random.sample(self.experience_buffer, self.batch_size)\n",
    "        \n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
    "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
    "        new_states = np.vstack([e.new_state for e in experiences if e is not None])\n",
    "        dones = np.vstack([e.done for e in experiences if e is not None])\n",
    "        \n",
    "        return (states, actions, rewards, dones, new_states)\n",
    "    \n",
    "    def warm_up(self, env):\n",
    "        for _ in range(10):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = env.action_space.sample()\n",
    "                new_state, reward, done, _ = env.step(action)\n",
    "                self.add_experience(state, action, reward, done, new_state)\n",
    "                state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain DQN algorithm\n",
    " * Similarities between tabular and neural networks\n",
    " * Similar action selection as earlier\n",
    " * Soft updates using target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import gym\n",
    "\n",
    "class Agent:    \n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.env = env\n",
    "        self.local_network = QNetwork(env, parameters).build_q_network()\n",
    "        self.target_network = QNetwork(env, parameters).build_q_network()\n",
    "        self.experience_replay = ExperienceReplay(parameters)\n",
    "        \n",
    "        self.epsilon = parameters[\"epsilon_init\"]\n",
    "        self.epsilon_decay = parameters[\"epsilon_decay\"]\n",
    "        self.epsilon_minimum = parameters[\"epsilon_minimum\"]\n",
    "        self.tau = parameters[\"tau\"]\n",
    "        self.gamma = parameters[\"gamma\"]\n",
    "        self.epochs = parameters[\"epochs\"]\n",
    "    \n",
    "    def learn(self):\n",
    "        states, actions, rewards, dones, next_states = self.experience_replay.get_batch()\n",
    "        \n",
    "        # Get Q-values for next state\n",
    "        Q_target = self.target_network.predict(next_states)\n",
    "\n",
    "        # Apply Q-learning algorithm to calculate the actual Q-value for state\n",
    "        Q_calc = rewards + (self.gamma * np.amax(Q_target, axis=1).reshape(-1, 1) * (1 - dones))\n",
    "        \n",
    "        # Calculate the predicted Q-value for the action taken in the state using local network\n",
    "        Q_local = self.local_network.predict(states)\n",
    "        \n",
    "        # Change Q_values for chosen action with \"correct\" Q-values, e.g. Q_actual        \n",
    "        for row, col_id in enumerate(actions):\n",
    "            Q_local[row, np.asscalar(col_id)] = Q_calc[row]\n",
    "        \n",
    "        # Network inputs states and outputs \n",
    "        self.local_network.fit(states, Q_local, epochs=self.epochs, verbose=0)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        local_weights = self.local_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "\n",
    "        for i in range(len(local_weights)):\n",
    "            target_weights[i] = self.tau * local_weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        self.target_network.set_weights(target_weights)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon >= self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def select_action(self, state):\n",
    "    \n",
    "        if self.epsilon > np.random.uniform():\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.local_network.predict(np.array([state])))\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def step(self, env, state):\n",
    "        \n",
    "        action  = self.select_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        return action, reward, done, new_state\n",
    "    \n",
    "    def save(self):\n",
    "        save_dir = os.path.join(os.getcwd(), env.spec.id +\"_\"+ datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "        os.makedirs(save_dir)\n",
    "        self.target_network.save(\"target_network.h5\")\n",
    "        self.local_network.save(\"local_network.h5\")\n",
    "        print(\"Weights saved successfully\")\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.target_network.load_weights(path + \"/target_network.h5\")\n",
    "        self.local_network.load_weights(path + \"/local_network.h5\")\n",
    "        print(\"Weights loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, iterations, episodes):\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_reward_list, iterations_list = [], []\n",
    "    agent.experience_replay.warm_up(env)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward=0\n",
    "        \n",
    "        \n",
    "        if (episode != 0): \n",
    "            agent.update_epsilon()\n",
    "    \n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            action, reward, done, new_state = agent.step(env, state)\n",
    "            agent.experience_replay.add_experience(state, action, reward, done, new_state)\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            agent.learn()\n",
    "            agent.update_target_network()\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "        \n",
    "        total_reward_list.append(total_reward)\n",
    "        iterations_list.append(iteration+1)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(\"Episode: {} | Average iterations: {} | Average total reward: {} | Epsilon: {} \" \\\n",
    "                  .format(episode, mean(iterations_list), mean(total_reward_list), agent.epsilon))\n",
    "            total_reward_list.clear()\n",
    "            iterations_list.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agent, env):\n",
    "    \n",
    "    done = False\n",
    "    agent.epsilon = 0\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action, reward, done, new_state = agent.step(env, state)\n",
    "        state = new_state\n",
    "        \n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(env):\n",
    "    \n",
    "    env_id = env.spec.id\n",
    "\n",
    "    if env_id == \"CartPole-v0\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"CartPole-v0\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.95,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.97,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 10000,\n",
    "            \"batch_size\" : 32,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "        \n",
    "    elif env_id == \"CartPole-v1\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"CartPole-v1\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.999,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "        \n",
    "    elif env_id == \"LunarLander-v2\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"LunarLander-v2\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.999,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2500,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "    else:\n",
    "        print(\"Standard hyperparameters {} chosen!\".format(env.spec.id))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.95,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.97,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 10000,\n",
    "            \"batch_size\" : 32,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "environment = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(environment)\n",
    "\n",
    "episodes = 3000\n",
    "iterations = 500\n",
    "parameters = get_hyperparameters(env)\n",
    "\n",
    "dqn_agent = Agent(env, parameters)\n",
    "train(dqn_agent, env, iterations, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run - Record - Show : One simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "monitor = gym.wrappers.Monitor(env, directory=\"videos\", force=True)\n",
    "\n",
    "#Why is three simulations needed for saving?!\n",
    "for _ in range(3):\n",
    "    play(dqn_agent, monitor)\n",
    "\n",
    "monitor.close()\n",
    "env.close()\n",
    "\n",
    "#HTML(\"\"\"\n",
    "#<video width=\"640\" height=\"480\" controls>\n",
    "#  <source src=\"{}\" type=\"video/mp4\">\n",
    "#</video>\n",
    "#\"\"\".format(\"./videos/\"+list(filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we want to plot\n",
    "from matplotlib import pyplot <br>\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.ion()\n",
    "\n",
    "fig.show()\n",
    "fig.canvas.draw()\n",
    "reward_list, episode_list, iteration_list = [], [], []\n",
    "\n",
    "episode_list.append(episode)\n",
    "reward_list.append(total_reward)\n",
    "\n",
    "if episode % 15 == 0:\n",
    "    ax.clear()\n",
    "    ax.plot(episode_list, reward_list)\n",
    "    fig.canvas.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
