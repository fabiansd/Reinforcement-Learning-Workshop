{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Deep Q-learning (DQN)\n",
    "Introduction to game we are going to play, e.g. visit website\n",
    "\n",
    "https://keon.io/deep-q-learning/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Q-Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to neural networks and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.metrics import MSE\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "class Q_Network:\n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.observations_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.loss_metric = parameters[\"loss_metric\"]\n",
    "        \n",
    "    \n",
    "    def build_Q_network(self, env):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.observations_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss=self.loss_metric, optimizer=Adam(lr=self.learning_rate))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "class ExperienceReplay:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.buffer_size = parameters[\"buffer_size\"]\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        self.experience_buffer = deque(maxlen=self.buffer_size)\n",
    "        \n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"new_state\", \"done\"])\n",
    "    \n",
    "    def add_experience(self, state, action, reward, new_state, done):   \n",
    "        e = self.experience(state, action, reward, new_state, done)\n",
    "        self.experience_buffer.append(e)\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \n",
    "        if len(self.experience_buffer) < self.batch_size:\n",
    "            experiences =  self.experience_buffer\n",
    "        else:\n",
    "            experiences = random.sample(self.experience_buffer, self.batch_size)\n",
    "        \n",
    "        #TODO Change this part?\n",
    "        states = np.hstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.hstack([e.action for e in experiences if e is not None])\n",
    "        rewards = np.hstack([e.reward for e in experiences if e is not None])\n",
    "        new_states = np.hstack([e.new_state for e in experiences if e is not None])\n",
    "        dones = np.hstack([e.done for e in experiences if e is not None])\n",
    "        \n",
    "        return (states, actions, rewards, new_states, dones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain DQN algorithm\n",
    " * Similarities between tabular and neural networks\n",
    " * Similar action selection as earlier\n",
    " * Soft updates using target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:    \n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.local_network = Q_Network(env, parameters).build_Q_network(env)\n",
    "        self.target_network = Q_Network(env, parameters).build_Q_network(env)\n",
    "        self.experience_replay = ExperienceReplay(parameters)\n",
    "        \n",
    "        self.epsilon = parameters[\"epsilon_init\"]\n",
    "        self.epsilon_decay = parameters[\"epsilon_decay\"]\n",
    "        self.tau = parameters[\"tau\"]\n",
    "    \n",
    "    def learn(self):\n",
    "        # THIS PART IS WRONG\n",
    "        experience_batch = self.experience_replay.get_batch()\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get best action\n",
    "        Q_targets_next = self.target_network.predict(next_states)\n",
    "        \n",
    "        # If done, Q_target should just be reward\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # \n",
    "        Q_expected = self.local_network.predict(states)\n",
    "        \n",
    "        self.local_network.fit(Q_expected, Q_targets)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        weights = self.local_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "        \n",
    "        target_weights = weights * self.tau + target_weights * (1-self.tau)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def select_action(self, state):\n",
    "    \n",
    "        if (self.epsilon > 0.01) and (self.epsilon > np.random.uniform()):\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.local_network.predict(state))\n",
    "\n",
    "        return action\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEED TO WORK ON DIMENTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "episodes = 100\n",
    "iterations = 50\n",
    "\n",
    "parameters = {\n",
    "    \"tau\" : 0.1,\n",
    "    \"epsilon_init\" : 1,\n",
    "    \"epsilon_decay\" : 0.95,\n",
    "    \"buffer_size\" : 3000,\n",
    "    \"batch_size\" : 30,\n",
    "    \"loss_metric\" : \"mse\",\n",
    "    \"learning_rate\" : 0.05}\n",
    "\n",
    "\n",
    "dqn_agent = Agent(env, parameters)\n",
    "state = env.reset()\n",
    "\n",
    "for episode in range(episodes):\n",
    "    \n",
    "    dqn_agent.update_epsilon()\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        action = dqn_agent.select_action(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run - Record - Show : One simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "monitor = gym.wrappers.Monitor(env, directory=\"videos\", force=True)\n",
    "\n",
    "#Why is three simulations needed for saving?!\n",
    "for _ in range(3):\n",
    "    done = True\n",
    "    monitor.reset()\n",
    "    \n",
    "    while not done:\n",
    "        # Select action\n",
    "\n",
    "monitor.close()\n",
    "env.close()\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+list(filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
