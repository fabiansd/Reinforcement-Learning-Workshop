{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning using tables\n",
    "##### Authors: Eirik Fagtun KjÃ¦rnli and Fabian Dietrichson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question for participants during the workshop\n",
    "\n",
    "#### Questions for us\n",
    "- How complex can each task be?\n",
    "- Which visualizations do we need?\n",
    "- How to make sure progress is upheld during the workshop?\n",
    "- How to make sure everybody manages to follow?\n",
    "\n",
    "- Hide assert cells?\n",
    "https://nbgrader.readthedocs.io/en/stable/user_guide/creating_and_grading_assignments.html\n",
    "\n",
    "Possible environments\n",
    "- taxi-v2 (https://medium.com/@anirbans17/reinforcement-learning-for-taxi-v2-edd7c5b76869)\n",
    "- #import gym.envs.toy_text -> env = gym.envs.toy_text.CliffWalkingEnv()\n",
    "\n",
    "#### Links\n",
    "Q-learning and introduction to the algorithm\n",
    "- Inspo: https://towardsdatascience.com/practical-reinforcement-learning-02-getting-started-with-q-learning-582f63e4acd9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Jupyter Notebooks and structure of notebook\n",
    "This workshop is structured such that for each cell you will write your code between .. blabal..\n",
    "\n",
    "#### Task:\n",
    "Click on the cell below, and run it to import the necessary libaries\n",
    "Hot key to run a cell: ctrl + enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_input_by_2(a_variabel):\n",
    "\n",
    "    \"Write code below\" \n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(multiply_input_by_2(10) == 20), \"Your method did not multiple the input by 2\"\n",
    "print(\"Great, you correctly implemented the method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of neccesary packages\n",
    "The first step to import the necessary packages. The gym package is the most central which contains the games which our Reinforcement learning agents are going to solve.\n",
    "\n",
    "#### Task:\n",
    "Click on the cell below and run it, to import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduce the game we are going to solve here\n",
    "\n",
    "Blabla this is how the game works\n",
    "\n",
    "#### Task:\n",
    "Create the environment variabel containing all necessary methods to run the Taxi-v2 game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_id = \"Taxi-v2\"\n",
    "\n",
    "env = gym.make(environment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "try:\n",
    "    assert(assert_env.reset())\n",
    "    print(\"You successfully created the environement {}\".format(env.spec.id))\n",
    "except:\n",
    "    raise Exception(\"You did not create the environment correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods we should create?\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MockData():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.seed(10)\n",
    "        \n",
    "    def get_Q(self):\n",
    "        Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q[0:2,0] = 10\n",
    "        return Q\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "\n",
    "mock = MockData()\n",
    "\n",
    "def visualize_q_table():\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "    heat_map = sb.heatmap(Q_trained)\n",
    "    plt.show()\n",
    "\n",
    "def store_data_for_visualization():\n",
    "    # To plot reward development\n",
    "    # Number of iterations\n",
    "    pass\n",
    "\n",
    "def reset_env_and_update_params(env):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    return state, done, iterations, total_reward\n",
    "\n",
    "def render_performance(Q):\n",
    "    iterations = 0\n",
    "    t_sleep = 1.2\n",
    "    \n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    time.sleep(t_sleep)\n",
    "    clear_output()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Get action\n",
    "        action = np.argmax(Q[state,:]) \n",
    "\n",
    "        # Take action in environment\n",
    "        new_state, _, done, _ = env.step(action)\n",
    "\n",
    "        # Update current state\n",
    "        state =  new_state\n",
    "\n",
    "        # Render\n",
    "        env.render()\n",
    "        time.sleep(t_sleep)\n",
    "        clear_output()\n",
    "        \n",
    "        # Increment iterations\n",
    "        iterations += 1\n",
    "        \n",
    "        if (iterations >= 25): done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Q-table\n",
    "Q-learning is one the most iconic Reinforcement learning algorihms, and can be used to solve a great variaty of challenges. In its most simplistic form it uses a table to store its values.\n",
    "\n",
    "Each row in the array is a state\n",
    "Each column is an action\n",
    "\n",
    "#### Task:\n",
    "Create and return a Q-table, where each cell is initialized to zeros, with:\n",
    "- The number of columns equal to number of actions in the environment (Use env.action_space.n)\n",
    "- The number of rows equal to number of states in the environment (Use env.observation_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_table(env):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "assert(np.count_nonzero(mock.get_Q() == 0)), \"All values in Q-table should be zero\"\n",
    "assert(mock_qtable().shape == (mock.get_env().observation_space.n, mock.get_env().action_space.n)), \\\n",
    "\"The dimensions are wrong\"\n",
    "print(\"The Q-table was correctly built! \" +\n",
    "      \"It has {} rows, each representing a unique state, and {} columns, each representing an action for that state.\"\\\n",
    "      .format(mock.get_env().observation_space.n, mock.get_env().action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select the best action\n",
    "The next method will pick out the best action given the state\n",
    "\n",
    "#### Task\n",
    "Pick the action with the highest Q-value given state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(Q, state):\n",
    "    \n",
    "    \"Write code below\" \n",
    "\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "assert(get_best_action(mock.get_Q(), 1) == 0), \"The method did not pick the action with the highest Q_value\"\n",
    "print(\"The best action for the test state was chosen, excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select an action\n",
    "The exploration vs. exploitation is a very effective method to ensure the agent explores a sufficent area of the state space, and avoid coverging to a local optima.\n",
    "\n",
    "#### Task:\n",
    "Compute the action to take in the current state, including exploration.  \n",
    "If the probability, e.g. epsilon, is higher than a random number, we should take a random action\n",
    "    otherwise - the best policy action (self.getPolicy).\n",
    "\n",
    "Tip: \n",
    "- To pick a random action, use env.action_space.sample()\n",
    "- To generate a random number with uniform probability, use np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    \n",
    "    \"Write code above\"    \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assert(select_action(mock.get_Q(), 1, 0) == 0), \\\n",
    "\"Method should always return the same value for this state, since epsilon is 0\"\n",
    "assert not (len(set([select_action(mock.get_Q(), 1, 1) for x in range(20)])) <= 2), \\\n",
    "\"Method should not return identical values when a random action, should be chosen\"\n",
    "print(\"The correct actions were picked, great job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradually shift towards exploitation, and reduce exploration of state space\n",
    "To make sure we are gradually moving from exploring the environment by taking random actions, we need to reduce the possibility of choosing a random action. In other words, we need to reduce Epsilon. \n",
    "\n",
    "#### Task:\n",
    "Create a method which reduces epsilon by a factor called epsilon decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon(epsilon):\n",
    "    epsilon_decay = 0.95\n",
    "    \n",
    "    \"Write code below\" \n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(update_epsilon(5) == 4.75), \"Given an input of 5, the output should have been 4.75\"\n",
    "print(\"Epsilon was correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Find the highest Q_value for a given state\n",
    " \n",
    " \n",
    " #### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_Q_value_in_state(Q, new_state):\n",
    "    \"Write code below\"\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(find_highest_Q_value_in_state(mock.get_Q(), 1) == 10), \\\n",
    "\"The method did not pick the action with the highest Q_value\"\n",
    "print(\"Perfect, the method returned the highest Q-value for that state!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate error  in Q-value\n",
    "\n",
    "Need to explain the discount factor\n",
    "\n",
    "#### Task:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_in_Q_value(Q, state, action, reward, new_state):\n",
    "    \n",
    "    discount_factor = 0.95\n",
    "    \n",
    "    \"Write code below\"\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return error_in_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(calculate_error_in_Q_value(mock.get_Q(), 0, 0, 10, 1) == 9.5), \\\n",
    "\"The method did not calculate the correct error value for the test sample\"\n",
    "print(\"The error in Q-value have been calculated correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Q-table\n",
    "\n",
    "Equation:\n",
    "- Q = Q + learning rate * error in Q-value\n",
    "\n",
    "#### Task\n",
    "Create a method which either sets\n",
    " - The Q-value equal to the reward, if it was the final episode. (Done is true)\n",
    " - Else updates the Q by following the equation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(Q, state, action, done, reward, new_state):\n",
    "    \n",
    "    learning_rate = 0.8\n",
    "    \n",
    "    \"Write code below\"\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(update_q_table(mock.get_Q(), 2, 1, 1, 10, 3)[2,1] == 10), \\\n",
    "\"The updated Q-value when simulation was done was incorrect\"\n",
    "assert(update_q_table(mock.get_Q(), 0, 0, 0, 10, 1)[0,0] == 17.6), \\\n",
    "\"The updated Q-value when the episode was not done, was incorrect\"\n",
    "print(\"Q-tabel have been correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take one step\n",
    "Take all we have learnt until this point\n",
    "\n",
    "#### Task\n",
    "    Chose action\n",
    "    Make step\n",
    "    Return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(Q, env, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return action, reward, done, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_env1 = deepcopy(mock.get_env())\n",
    "mock_env1.reset()\n",
    "mock_env2 = deepcopy(mock_env1)\n",
    "\n",
    "new_state, _, _, _ = mock_env1.step(0)\n",
    "assert(step(mock.get_Q(), mock_env2, 0, 0) == (0, -1, False, new_state)), \\\n",
    "\"Your environment did not return the correct values\"\n",
    "print(\"Your step method seems to function properly, good job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train method\n",
    "\n",
    "Explain what an episode and iteration is\n",
    "Explain code already writen\n",
    "\n",
    "#### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, num_episodes, epsilon = 1):\n",
    "    done = False\n",
    "    \n",
    "    \"Write code below\"\n",
    "    # Initialize Q-table\n",
    "    \n",
    "    \"Write code above\"\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        if (done and episode % 10 == 0): \n",
    "            print(\"Episode: {} | Iterations: {} | Total Reward: {}\".format(episode, iterations, total_reward))\n",
    "        \n",
    "        state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "        \n",
    "        \"Write code below\"\n",
    "        # Update epsilon\n",
    "        \n",
    "        \n",
    "        \"Write code above\"\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            \"Write code below\"\n",
    "            \n",
    "            # Take step\n",
    "            \n",
    "                        \n",
    "            # Update Q-table\n",
    "            \n",
    "            \n",
    "            # Set the new state as the current state\n",
    "            \n",
    "            \n",
    "            \"Write code above\"\n",
    "            \n",
    "            # Update episode information\n",
    "            iterations += 1\n",
    "            total_reward += reward\n",
    "            \n",
    "            # End episode if it has lasted for too many iterations\n",
    "            if (iterations >= 40): done = True\n",
    "\n",
    "    return Q, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create play method\n",
    "\n",
    "\n",
    "#### Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, Q):\n",
    "    \n",
    "    state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        \"Write code below\"\n",
    "        # Take step\n",
    "        \n",
    "\n",
    "        # Set the new state as the current state\n",
    "        \n",
    "\n",
    "        \"Write code above\"\n",
    "\n",
    "        # Update episode information\n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        if (iterations >= 25): done = True\n",
    "    \n",
    "    print(\"Iterations: {} | Total Reward: {}\".format(iterations, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time to actually use our methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_trained, _ = train(env, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, Q_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_performance(Q_trained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
