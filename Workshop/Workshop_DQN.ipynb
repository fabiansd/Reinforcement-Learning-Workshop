{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson [Accenture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Reinforcement learning has resulted in superman performance in a range of scenarios, such as Chess, Atari 2600, and Starcraft, but also in robotics. The main reason for this success compared to traditional Reinforcement Learning is the introduction of deep neural networks. The first successful implementation of deep neural networks in combination with a Reinforcement learning algorithm was Mnih et. al. which was able to reach superhuman performance in the classic Atari games. Their approach was coined Deep Q-Networks(DQN), which is very similar to the Q-learning algorithm we implemented in the previous workshop, in combination with deep neural networks. This is the algorithm we will be implementing in this notebook.\n",
    "\n",
    "In the DQN approach, a neural network replaces the Q-table which we used in the previous notebook. The main reason for this is the neural networks ability to generalize around similar states. In a continuous environment, a Q-table would quickly grow to tens of millions of unique states, although many of these are so similar that the same action should be applied. This is both an issue due to the huge table we would need, but also because we would need a tremendous amount of training data to update all the states. \n",
    "\n",
    "Neural networks address this issue, however, they do come with their own challenges. Neural network struggles with instabilities during training, in addition to critical forgetting and divergence. Mnih's success was due to several ingenious tricks which stabilized the network during training, and we will go through these during the workshop.\n",
    "\n",
    "In the previous notebook, we created a lot of methods, which resulted in a lot of parameters being passed from method to method. This can quickly become convoluted, and in this notebook we instead use classes. An example can be seen below. When the class is created the init method is run, and the different parameters are set and stored in this instance of the class. The parameters can be called within the class using \"self.<name_of_parameter>\" syntax, e.g. self.learning_rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and create support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in addition to the support function that will be used throughout the workshop. \n",
    "\n",
    "### Task 1\n",
    "Import the packages needed for this workshop, simply mark the cell below and press CTRL + Enter\n",
    "\n",
    "### Task 2\n",
    "Create the necessary support methods by running the second cell below. Mark it and press CTRL + Enter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "import gym\n",
    "import os\n",
    "import datetime\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "from tensorflow.python.keras import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.metrics import MSE\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.initializers import glorot_uniform\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "%precision 3\n",
    "should_assert = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_blank_weights(layer_dims):\n",
    "    \n",
    "    # Xavier/Glorot Initialization\n",
    "    new_weights = np.random.randn(layer_dims[0], layer_dims[1])*np.sqrt(1/layer_dims[0])\n",
    "    new_bias = np.zeros((layer_dims[1]))\n",
    "    \n",
    "    return [new_weights, new_bias]\n",
    "\n",
    "def create_dummybuffer():\n",
    "    parameters = {\"buffer_size\": 6,\n",
    "                  \"batch_size\": 3}\n",
    "    DummyBuffer = ExperienceReplay(parameters)\n",
    "    \n",
    "    return DummyBuffer\n",
    "\n",
    "def get_dummy_parameters_and_env():\n",
    "        \n",
    "        parameters = {\n",
    "            \"tau\" : 0.4,\n",
    "            \"gamma\" : 1,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 1,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 5,\n",
    "            \"hidden_layer_2\": 5}\n",
    "        \n",
    "        dummy_env = gym.make(\"CartPole-v0\")\n",
    "        return parameters, dummy_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Q-Networks\n",
    "There are several frameworks which can be used to create neural networks such as Tensorflow from Google, PyTorch from Facebook or MXNet from Apache. In this notebook, we will be using Tensorflow, with a high-level API called Keras on top. Keras greatly simplifies the effort needed to create and train a neural network, and it is often referred to as the best Deep Learning framework for those who are just starting with neural networks.\n",
    "\n",
    "Neural networks are partially inspired by the structure of the human brain and consist of a network of interconnected neurons. The neural network consists of an input layer, a set of hidden layers, and an output layer, as shown in the figure below.\n",
    "\n",
    "<img src=\"Images/Neural_network__achitecture.svg\" alt=\"drawing\" width=\"400\" height=\"200\"/>\n",
    "\n",
    "\n",
    "\n",
    "### Task\n",
    "Now we will see how easy it is to create a neural network using Keras. We are going to create a neural network with an input layer, two hidden layers, and one output layer. The design criterions are as follows: <br>\n",
    "\n",
    "1. The model should be fully connected \n",
    "<br>\n",
    "<br>\n",
    "2. The input layer and first hidden layer is created together, and should:\n",
    "    - Be of type Dense\n",
    "    - Use the ReLU activation function\n",
    "    - Have input size equal to the observations size of the environment\n",
    "    - Have hidden layers size equal to parameter hidden_layer_1 \n",
    "<br>\n",
    "<br>\n",
    "3. The second hidden layer should:\n",
    "    - Be of type Dense\n",
    "    - Use the ReLU activation function \n",
    "    - The hidden layer should have size equal to parameter hidden_layer_2\n",
    "    - Tip1: This layer is similar to previous, but without the input_dim parameter\n",
    "    - Tip2: Remember to add the dense layer using model.add()\n",
    "<br>\n",
    "<br>\n",
    "4. The output layer should:\n",
    "    - Be of type Dense\n",
    "    - The activation function should be linear\n",
    "    - Should have size equal to action vector, i.e. action_size\n",
    "<br>\n",
    "<br>\n",
    "5. The final part is to compile the network. Before we do this we must define som parameters\n",
    "    - Loss metric should be Mean-squared-error (MSE)\n",
    "    - The optimizer should be Adaptive Moment Estimation (Adam)\n",
    "    - Learning rate should equal to learning_rate\n",
    "    - Learning rate decay should be equal to learning_rate_decay\n",
    "<br>\n",
    "As you can see below step 1,2 and 5 is already done, so your task is to complete task 3 and 4.\n",
    "\n",
    "**Ps!** <br>\n",
    "At the end of the workshop, you can experiment with different structures and parameters, however, in the first walkthrough, you will follow the defined steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.observations_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.learning_rate = parameters[\"learning_rate\"]\n",
    "        self.learning_rate_decay = parameters[\"learning_rate_decay\"]\n",
    "        self.loss_metric = parameters[\"loss_metric\"]\n",
    "        self.hidden_layer_1 = parameters[\"hidden_layer_1\"]\n",
    "        self.hidden_layer_2 = parameters[\"hidden_layer_2\"]    \n",
    "        \n",
    "    def build_q_network(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(self.hidden_layer_1, input_dim=self.observations_size, activation='relu'))\n",
    "        \n",
    "        \"Input code below\"\n",
    "        \n",
    "        \"Input code above\"\n",
    "        \n",
    "        model.compile(loss=self.loss_metric, optimizer=Adam(lr=self.learning_rate, decay=self.learning_rate_decay))\n",
    "    \n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if(should_assert):\n",
    "    mock_parameters = {\"loss_metric\" : \"mse\",\n",
    "                \"learning_rate\" : 0.01,\n",
    "                \"learning_rate_decay\": 0.01,\n",
    "                \"hidden_layer_1\": 24,\n",
    "                \"hidden_layer_2\": 24}\n",
    "\n",
    "    mock_env = gym.make(\"CartPole-v0\")\n",
    "    mock_q_network = QNetwork(mock_env, mock_parameters)\n",
    "    mock_model = mock_q_network.build_q_network()\n",
    "    config_network = mock_model.get_config()\n",
    "    assert(config_network.get(\"name\")[0:10] == \"sequential\")\n",
    "\n",
    "    # Layers\n",
    "    config_layers = config_network.get(\"layers\")\n",
    "    assert(len(config_layers) == 3), \"You should only have 2 dense layers\"\n",
    "\n",
    "    # First layer\n",
    "    assert(config_layers[0].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[0].get(\"config\").get(\"batch_input_shape\") == (None, mock_q_network.observations_size))\n",
    "    assert(config_layers[0].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[0].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
    "    \"Activation function for first layer should be relu\"\n",
    "\n",
    "    # Second layer\n",
    "    assert(config_layers[1].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[1].get(\"config\").get(\"units\") == 24), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[1].get(\"config\").get(\"activation\") == \"relu\"), \\\n",
    "    \"Activation function for second layer should be relu\"\n",
    "\n",
    "    # Thrid layer\n",
    "    assert(config_layers[2].get(\"class_name\") == \"Dense\"),\"Incorrect layertype in first layer\"\n",
    "    assert(config_layers[2].get(\"config\").get(\"units\") == mock_q_network.action_size), \"Incorrect number of neurons in first layer\"\n",
    "    assert(config_layers[2].get(\"config\").get(\"activation\") == \"linear\"),\\\n",
    "    \"Activation function for third layer should be linear\"\n",
    "\n",
    "    config_optimizer = mock_model.optimizer.get_config()\n",
    "    assert(0.0099 < config_optimizer.get(\"lr\") <= 0.01),\"Learning rate should be 0.01\"\n",
    "    assert(0.0099 < config_optimizer.get(\"decay\") <= 0.01),\"Learning rate decay should be 0.01\"\n",
    "    assert(mock_model.loss == \"mse\"), \"Loss metric should me mse\"\n",
    "    \n",
    "    print(\"Superb, you implemented the layers correct! Information about your model is shown below.\\n\")\n",
    "    print(\"PS. The input layer is not shown, altough you should be able to calculate it by looking at <Param #>\")\n",
    "    print(\" of the first hidden layer.\\n\")\n",
    "    mock_model.summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Experience Buffer\n",
    "\n",
    "For a neural network to perform optimally we want the data to be I.I.D (Independent and Identically Distributed). In supervised learning, where for instance the network is fed an image and it will predict either cat or dog, this is achieved by randomly sampling the training data from the full data set. As a result:\n",
    "- Batches have close-to similar data distribution.\n",
    "- Samples in each batch are independent of each other.\n",
    "\n",
    "In Reinforcement learning where our agent samples data by moving from state to state, the recently sampled data will be highly correlated to each other. As a result, we will feed our network data which closely resembles each other, which is a recipe for disaster when working with a neural network. \n",
    "\n",
    "Furthermore, the data distribution of the initial states will be different from that of the later stage, i.e. this does not satisfy the I.I.D criterion. This is bad news!\n",
    "\n",
    "Luckily Mnih et. al. has a solution. Instead of training on the just the previously collected samples, we store all states in an **Experience Buffer** and sample randomly from this buffer.\n",
    "\n",
    "### Warm-up\n",
    "The Experience Buffer is a collection of all the experiences the agent has collected during its interaction with the environment. As a result, the experience buffer is empty at the begin of the first episode. This is not an issue, as you in the previous task implemented the necessary logic to handle the case when there are fewer experience_touples in the buffer than the batch size. However, this does slow down training as we train with few and highly correlated samples. \n",
    "\n",
    "To counteract this issue, we introduce a method called warm_up. The warm_up method performs completes a certain amount of episodes taking random actions, thereby creating reducing the correlation between samples in the initial training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "\n",
    "This section is divided into multiple tasks. You will first complete task 1, and then run the first assertion cell for task 1. Then you will continue to task 2 and run assertion cell 2 etc.\n",
    "\n",
    "### Task 1\n",
    "\n",
    "The first method we are going to create is a method which adds an experience, i.e. a state-transition. This is stored as a tuple which contains the following fields; state, action, reward, done, new_state.\n",
    "\n",
    "To store a state-transition we are going to use a specific type of tuple, namely a namedtuple. You can read more about it [here](https://docs.python.org/2/library/collections.html#collections.namedtuple). We have created a namedtuple which you are going to use called experience_tuple. The experience_tuple have the following field where you can store information; state, action, reward, done, new_state. To create a state-transition do the following:\n",
    "- self.experience_tuple(state=your_state, action=your_action, reward=your_reward, done=your_done, new_state=your_new_state)\n",
    "\n",
    "To add a named_tuple to the buffer use the append method of the experience_buffer:\n",
    "- self.experience_buffer.append(your_experience_tuple)\n",
    "\n",
    "\n",
    "### Task 2\n",
    "\n",
    "The next method we are going to create is a method which returns a batch of experience_tuples, which is used during training. The methods should set the variable *experiences* equal to either\n",
    "- All the samples in the experience_buffer IF it is less than the batch size\n",
    "- or ELSE a *batch_size* of randomly sample experience_tuples from the experience_buffer. To randomly sample use the random.sample() method. You can read more about the method [here](https://docs.python.org/2/library/random.html).\n",
    "\n",
    "The last part of the method is created by us, and it creates vectors of all elements of each field. This is done to vectorize the training and gives a significant reduction in training time.\n",
    "\n",
    "### Task 3\n",
    "\n",
    "The final method we will create is the warm-up method we introduced earlier. We have created the high-level structure, but you will create the code which completes one episode. The method should do the following\n",
    "1. Use a While-loop which runs as long as the Done parameter is \"not true\"\n",
    "2. Chooses a random action - Use env.action_space.sample()\n",
    "3. Use the random action to make a step in the environment - Use env.step(random_action).\n",
    "4. Add the experience to the replay_buffer - Use the add_experience method we created earlier\n",
    "5. Remember to update the current state, to the new_state, e.g. state = new_state\n",
    "\n",
    "Remember to test your method by running the assertion cell for task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    \n",
    "    def __init__(self, parameters):\n",
    "        self.buffer_size = parameters[\"buffer_size\"]\n",
    "        self.batch_size = parameters[\"batch_size\"]\n",
    "        self.experience_buffer = deque(maxlen=self.buffer_size)\n",
    "        \n",
    "        self.experience_tuple = namedtuple(\"Experience\", \n",
    "                                           field_names=[\"state\", \"action\", \"reward\", \"done\", \"new_state\"])\n",
    "    \n",
    "    def add_experience(self, state, action, reward, done, new_state):\n",
    "        \n",
    "        \"Input code below\"\n",
    "        \n",
    "        \"Input code above\"\n",
    "        \n",
    "    def get_batch(self):\n",
    "        \n",
    "        \"Input code below\"\n",
    "        \n",
    "        \"Input code above\"\n",
    "        \n",
    "        \n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
    "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
    "        new_states = np.vstack([e.new_state for e in experiences if e is not None])\n",
    "        dones = np.vstack([e.done for e in experiences if e is not None])\n",
    "        \n",
    "        return (states, actions, rewards, dones, new_states)\n",
    "    \n",
    "    def warm_up(self, env):\n",
    "        for _ in range(10):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            \n",
    "            \"Input code below\"\n",
    "            \n",
    "            \"Input code above\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertion - Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if should_assert:\n",
    "    DummyBuffer = create_dummybuffer()\n",
    "\n",
    "    for i in range(6):\n",
    "        DummyBuffer.add_experience(i, i%2, i, i%2, i+1)\n",
    "\n",
    "    dummy_experience_buffer = DummyBuffer.experience_buffer.copy()\n",
    "    assert(len(dummy_experience_buffer) == 6),\\\n",
    "    \"Length of your experience buffer is wrong, should be 6, was {}\".format(len(dummy_experience_buffer))\n",
    "\n",
    "    for e in range(6):\n",
    "        dummy_experience = DummyBuffer.experience_tuple(state=e, action=e%2, reward=e, done=e%2, new_state=e+1)\n",
    "        stored_experience = dummy_experience_buffer.popleft()\n",
    "        assert(dummy_experience == stored_experience), \\\n",
    "        \"The values were incorrectly stored, was {}, should have been {}\".format(stored_experience, dummy_experience)\n",
    "\n",
    "    print(\"You implemented the add_experience method correctly, great job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertion - Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if should_assert:\n",
    "    DummyBuffer = create_dummybuffer()\n",
    "    for i in range(2):\n",
    "        DummyBuffer.add_experience(i, i%2, i, i%2, i+1)\n",
    "\n",
    "    assert(len(DummyBuffer.get_batch()[0]) == 2),\\\n",
    "    \"Should return 2 samples, when there are less experience_tuples than the batch_size. Your method returned {} samples\"\\\n",
    "    .format(len(DummyBuffer.get_batch()[0]))\n",
    "\n",
    "    DummyBuffer.add_experience(3, 3%2, 3, 3%2, 3+1)\n",
    "    dummy_batch = DummyBuffer.get_batch()\n",
    "    assert(len(dummy_batch[0]) == 3),\\\n",
    "    \"Should return 3 samples, when there are 3 experience_tuples in the experience_buffer. Your method returned {} samples\"\\\n",
    "    .format(len(DummyBuffer.get_batch()[0]))\n",
    "\n",
    "\n",
    "    DummyBuffer.add_experience(4, 4%2, 4, 4%2, 4+1)\n",
    "    dummy_batch = DummyBuffer.get_batch()\n",
    "    assert(len(dummy_batch[0] == 3)),\\\n",
    "    \"Should return 3 samples, when there are more experience_tuples than the batch_size. Your method returned {} samples\"\\\n",
    "    .format(len(DummyBuffer.get_batch()[0]))\n",
    "\n",
    "    assert(len(np.unique(dummy_batch[0][0:3])) == 3), \"The method did not return the values correctly, states should be all unique values. You returned {}\".format(dummy_batch[0])\n",
    "    assert(len(np.unique(dummy_batch[-1][0:3])) == 3), \"The method did not return the values correctly, new_states should be all unique values. You returned {}\".format(dummy_batch[-1])\n",
    "\n",
    "    print(\"Great, you implemented the get_batch() method correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assertion Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if should_assert:\n",
    "    parameters = {\"buffer_size\": 1000,\n",
    "                  \"batch_size\": 1}\n",
    "    mock_env1 = gym.make(\"CartPole-v1\")\n",
    "    mock_experience_replay = ExperienceReplay(parameters)\n",
    "    mock_env1.reset()\n",
    "    mock_env2 = deepcopy(mock_env1)\n",
    "\n",
    "    mock_experience_replay.warm_up(mock_env1)\n",
    "    mock_state = mock_env2.reset()\n",
    "    while 0 < len(mock_experience_replay.experience_buffer):\n",
    "        mock_action = mock_env2.action_space.sample()\n",
    "        mock_new_state, mock_reward, mock_done, _ = mock_env2.step(mock_action)\n",
    "        \n",
    "        mock_experience_tuple = mock_experience_replay.experience_buffer.popleft()\n",
    "        np.testing.assert_array_equal(mock_experience_tuple[0], mock_state, \"State was incorrect\")\n",
    "        assert(mock_experience_tuple[1] == mock_action), \"Action was incorrect\"\n",
    "        assert(mock_experience_tuple[2] == mock_reward), \"Reward was incorrect\"\n",
    "        np.testing.assert_array_equal(mock_experience_tuple[4], mock_new_state, \"New_state was incorrect\")\n",
    "        \n",
    "        if mock_done:\n",
    "            mock_state = mock_env2.reset()\n",
    "        else:\n",
    "            mock_state = mock_new_state\n",
    "            \n",
    "    print(\"Your warm-up method seems to function properly, good job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent\n",
    "\n",
    "### Target network\n",
    "\n",
    "In the supervised learning example, where we are to predict cat or dog, the labels are stationary, e.g. an image of a cat will always have the label cat. This is not the case in Reinforcement learning.\n",
    "\n",
    "As we experienced in the previous notebook, the Q-value of the next states is calculated based on the assumption that the Q-table is correct. However, since we begin each training session with a blank table and update the values as we go along, the Q-values of the next state is just an estimate. As a result, we are chasing a moving target, which degrades the training of the neural network and may lead to divergence.\n",
    "\n",
    "Mnih et. al. solved this by using a target network to choose action and to predict the Q-values of the next state $S_{t+1}$, and used a local network to predict the Q-values of state $S_{t}$. The two networks have an identical structure. To update the target network, the equation below is used, where $\\theta$ represents the weights of each network.\n",
    "\n",
    "$\n",
    "\\theta_{target} = \\tau * \\theta_{local} + (1-\\tau) * \\theta_{target}\n",
    "$\n",
    "\n",
    "### Task 1\n",
    "Implement the target network algorithm in the \"update_target_network\" method below. We have already created the shell, where we loop through each layer of the network. Your task\n",
    "- Update the weights of each layer,  by using the algorithm above. Remember to use the self.tau parameter.\n",
    "- Remember to run the assertion cell\n",
    "\n",
    "### Q-learning algorithm\n",
    "\n",
    "\n",
    "### Task 2\n",
    "Go through the Q-learning algorithm, and compare it to the Q-learning algorithm we saw in the last notebook. Make sure you understand the implementation before moving on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:    \n",
    "\n",
    "    def __init__(self, env, parameters):\n",
    "        self.env = env\n",
    "        self.q_network = QNetwork(env, parameters)\n",
    "        self.local_network = self.q_network.build_q_network()\n",
    "        self.target_network = self.q_network.build_q_network()\n",
    "        self.experience_replay = ExperienceReplay(parameters)\n",
    "        \n",
    "        self.epsilon = parameters[\"epsilon_init\"]\n",
    "        self.epsilon_decay = parameters[\"epsilon_decay\"]\n",
    "        self.epsilon_minimum = parameters[\"epsilon_minimum\"]\n",
    "        self.tau = parameters[\"tau\"]\n",
    "        self.gamma = parameters[\"gamma\"]\n",
    "        self.epochs = parameters[\"epochs\"]\n",
    "\n",
    "    def update_local_network(self):\n",
    "        states, actions, rewards, dones, next_states = self.experience_replay.get_batch()\n",
    "        \n",
    "        # Get Q-values for the next state, Q(next_state), using the target network\n",
    "        Q_target = self.target_network.predict(next_states)\n",
    "\n",
    "        # Apply Q-learning algorithm and Q-value for next state to calculate the actual Q-value the Q(state)\n",
    "        Q_calc = rewards + (self.gamma * np.amax(Q_target, axis=1).reshape(-1, 1) * (1 - dones))\n",
    "        \n",
    "        # Calculate Q-value Q(state) we predicted earlier using the local network\n",
    "        Q_local = self.local_network.predict(states)\n",
    "        \n",
    "        # Update Q_values with \"correct\" Q-values calculated using the Q-learning algorithm      \n",
    "        for row, col_id in enumerate(actions):\n",
    "            Q_local[row, np.asscalar(col_id)] = Q_calc[row]\n",
    "        \n",
    "        # Train network by minimizing the difference between Q_local and modified Q_local\n",
    "        self.local_network.fit(states, Q_local, epochs=self.epochs, verbose=0)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        local_weights = self.local_network.get_weights()\n",
    "        target_weights = self.target_network.get_weights()\n",
    "\n",
    "        for layer in range(len(local_weights)):\n",
    "            \n",
    "            \"Input code below\"\n",
    "        \n",
    "            \"Input code above\"\n",
    "        \n",
    "        self.target_network.set_weights(target_weights)\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "        if self.epsilon >= self.epsilon_minimum:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "        \n",
    "    def select_action(self, state):\n",
    "    \n",
    "        if self.epsilon > np.random.uniform():\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(self.local_network.predict(np.array([state])))\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def step(self, env, state):\n",
    "        \n",
    "        action  = self.select_action(state)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        return action, reward, done, new_state\n",
    "    \n",
    "    def save(self):\n",
    "        save_dir = os.path.join(os.getcwd(), env.spec.id +\"_\"+ datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
    "        os.makedirs(save_dir)\n",
    "        self.target_network.save(\"target_network.h5\")\n",
    "        self.local_network.save(\"local_network.h5\")\n",
    "        print(\"Weights saved successfully\")\n",
    "        \n",
    "    def load(self):\n",
    "        uri = \"freeze_weights/\" + env.spec.id + \"/\"\n",
    "        self.target_network.load_weights(uri + \"target_network.h5\")\n",
    "        self.local_network.load_weights(uri + \"local_network.h5\")\n",
    "        print(\"Weights loaded successfully for environment: {}\".format(env.spec.id))\n",
    "        \n",
    "    def freeze_network(self, freeze_layers):\n",
    "        # Freezes first freeze_layers of the network and resets the other if freeze_layers is not zero.\n",
    "        network_size = len(self.local_network.get_config().get(\"layers\"))\n",
    "        assert(freeze_layers <= network_size),\\\n",
    "        \"Tried to freeze more layers than there are layers in local network!\"\n",
    "        \n",
    "        self.load()\n",
    "        \n",
    "        if freeze_layers == 0:\n",
    "            print(\"No layers frozen, using loaded weights\")\n",
    "        elif freeze_layers == network_size:\n",
    "            for layer in range(network_size):\n",
    "                self.local_network.layers[layer].trainable = False \n",
    "            print(\"Frozen all layers, and using loaded weights\")\n",
    "        else:             \n",
    "            for layer in range(network_size):\n",
    "                if layer < freeze_layers:\n",
    "                    self.local_network.layers[layer].trainable = False \n",
    "                else:\n",
    "                    new_weights = generate_blank_weights(self.local_network.layers[layer].get_weights()[0].shape)\n",
    "                    self.local_network.layers[layer].set_weights(new_weights)\n",
    "                    self.target_network.layers[layer].set_weights(new_weights)    \n",
    "            \n",
    "            self.local_network.compile(loss=self.q_network.loss_metric, \n",
    "                          optimizer=Adam(lr=self.q_network.learning_rate, \n",
    "                                         decay=self.q_network.learning_rate_decay))\n",
    "            print(\"Networks first {} layers are succesfully frozen\".format(freeze_layers))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if should_assert:\n",
    "    dummy_parameters, dummy_env = get_dummy_parameters_and_env()\n",
    "    dummy_dqn = Agent(dummy_env, dummy_parameters)\n",
    "\n",
    "    dummy_local_weights = deepcopy(dummy_dqn.local_network.get_weights())\n",
    "    dummy_target_weights = deepcopy(dummy_dqn.target_network.get_weights())\n",
    "\n",
    "    dummy_dqn.update_target_network()\n",
    "    for i in range(len(dummy_target_weights)):\n",
    "        np.testing.assert_array_equal(\n",
    "            dummy_dqn.target_network.get_weights()[i], \n",
    "            dummy_parameters.get(\"tau\") * dummy_local_weights[i] + (1 - dummy_parameters.get(\"tau\")) * dummy_target_weights[i],\n",
    "            err_msg=\"\\nThe target network was not implemented correctly. Layer {} was incorrect\\n\".format(i))\n",
    "\n",
    "    print(\"Great job, you implemented the update_target_network-method correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "The training method is more or less identical to the method we used in the previous notebook, with some modifications due to the introduction of a neural network to represent the Q-table.\n",
    "\n",
    "\n",
    "### Task\n",
    "Make sure you understand the method before moving on!\n",
    "- Do you understand why the episodic reward is a better metric of the neural network's performance than the loss-metric when doing Reinforcement learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, env, iterations, episodes):\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_reward_list, iterations_list = [], []\n",
    "    agent.experience_replay.warm_up(env)\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward=0\n",
    "        \n",
    "        if (episode != 0): \n",
    "            agent.update_epsilon()\n",
    "    \n",
    "        for iteration in range(iterations):\n",
    "            \n",
    "            action, reward, done, new_state = agent.step(env, state)\n",
    "            agent.experience_replay.add_experience(state, action, reward, done, new_state)\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "            agent.update_local_network()\n",
    "            agent.update_target_network()\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done: \n",
    "                break\n",
    "        \n",
    "        total_reward_list.append(total_reward)\n",
    "        iterations_list.append(iteration+1)\n",
    "        \n",
    "        if episode % 10 == 0 and episode != 0:\n",
    "            print(\"Episode: {0:d}-{1:d} | Average iterations: {2:0.2f}  | Average total reward: {3:0.2f} | Epsilon: {4:0.4f}\" \\\n",
    "                  .format(episode-10, episode, mean(iterations_list), mean(total_reward_list), agent.epsilon))\n",
    "            total_reward_list.clear()\n",
    "            iterations_list.clear()\n",
    "        \n",
    "    return agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play\n",
    "The logic in the play method is identical to the method we used in the previous method.\n",
    "The method completes one episode using the inputted agent-object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agent):\n",
    "    \n",
    "    done = False\n",
    "    agent.epsilon = 0\n",
    "    total_reward = 0\n",
    "    env = agent.env\n",
    "    state = env.reset()\n",
    "    \n",
    "    while not done:\n",
    "        action, reward, done, new_state = agent.step(env, state)\n",
    "        state = new_state\n",
    "        \n",
    "        total_reward += reward\n",
    "    \n",
    "    print(\"Total Reward: {}\".format(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "The hyperparameters are the customizable parameters which are not learned by the agent but are set by us.\n",
    "\n",
    "The number of parameters passed to the agent significantly increases when we begin using neural networks. To simplify the process, we define them in a dictionary and simply pass the dictionary to the agent. We have chosen the hyperparameters for some environments, and they should not be changed the first time you go through the notebook.\n",
    "\n",
    "PS. You may change them as you please when you have passed through the notebook once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(env):\n",
    "    \n",
    "    env_id = env.spec.id\n",
    "\n",
    "    if env_id == \"CartPole-v0\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"CartPole-v0\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.95,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.97,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 10000,\n",
    "            \"batch_size\" : 32,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "        \n",
    "    elif env_id == \"CartPole-v1\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"CartPole-v1\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.99, #0.999\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2000,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "        \n",
    "    elif env_id == \"LunarLander-v2\":\n",
    "        print(\"Hyperparameters for {} chosen!\".format(\"LunarLander-v2\"))\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.99,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.999,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 2500,\n",
    "            \"batch_size\" : 64,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "    else:\n",
    "        print(\"Standard hyperparameters chosen!\")\n",
    "        parameters = {\n",
    "            \"tau\" : 0.05,\n",
    "            \"gamma\" : 0.95,\n",
    "            \"epsilon_init\" : 1,\n",
    "            \"epsilon_decay\" : 0.97,\n",
    "            \"epsilon_minimum\": 0.01,\n",
    "            \"buffer_size\" : 10000,\n",
    "            \"batch_size\" : 32,\n",
    "            \"epochs\": 1,\n",
    "            \"loss_metric\" : \"mse\",\n",
    "            \"learning_rate\" : 0.01,\n",
    "            \"learning_rate_decay\": 0.01,\n",
    "            \"hidden_layer_1\": 24,\n",
    "            \"hidden_layer_2\": 24}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = \"CartPole-v1\"\n",
    "episodes = 5\n",
    "iterations = 500\n",
    "layers_to_freeze = 1\n",
    "\n",
    "env = gym.make(environment)\n",
    "parameters = get_hyperparameters(env)\n",
    "\n",
    "dqn_agent = Agent(env, parameters)\n",
    "dqn_agent.freeze_network(layers_to_freeze)\n",
    "dqn_agent = train(dqn_agent, env, iterations, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test trained agent for CartPole-v1\n",
    "For the CartPole-v0 environment the maximum score is 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(dqn_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run - Record - Show : One simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "monitor = gym.wrappers.Monitor(env, directory=\"videos\", force=True)\n",
    "\n",
    "for _ in range(3):\n",
    "    play(dqn_agent, monitor)\n",
    "\n",
    "monitor.close()\n",
    "env.close()\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "#\n",
    "\"\"\".format(\"./videos/\"+list(filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
