{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning using tables\n",
    "##### Authors: Eirik Fagtun Kj√¶rnli and Fabian Dietrichson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome \n",
    "Welcome to this workshop!\n",
    "\n",
    "The workshop is structured such that for each cell, you will write your own code and the code will then be asserted in the next cell. The assertion cell is marked \"# Do not edit - Assertion cell #\". <br>\n",
    "The code should be written between \"Write code below\" and \"Write code above\". If your code does not pass the assertion, you will have to rewrite it before continuing.\n",
    "\n",
    "### Task:\n",
    "Implment the method multiply_input_by_2\n",
    "- The code should take the input variabel \"input_variabel\", and multiply it by 2\n",
    "- The answer should be stored in the variabel \"result\"\n",
    "- When the method is implemented, run the cell.\n",
    "\n",
    "Hotkey to run a cell: CTRL + ENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_input_by_2(input_variabel):\n",
    "\n",
    "    \"Write code below\" \n",
    "    result = input_variabel * 2\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "assert(multiply_input_by_2(10) == 20), \"Your method did not multiple the input by 2\"\n",
    "print(\"Great, you correctly implemented the method!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages\n",
    "To implement the native Q-learning algorithm and use it in an environment, we just need two additional packages.\n",
    "\n",
    "_Numpy_\n",
    "- Numpy adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays. We could do this workshop using native Python arrays and built-in methods, however using Numpy simplifies the process. \n",
    "- To call the methods in the package, we use the \"as np\" command. Such that, to create a numpy table with 2 rows and 2 columns where all the cells are zero, you can simply write np.seros(2,2).\n",
    "\n",
    "_OpenAI Gym_\n",
    "- The Gym library is a collection of demo problems an associated environment, that you can use to work out your reinforcement learning algorithms. These range from simple text problems, to complex physical problems, to Atari video games. OpenAI Gym is therefore by many the preferred framework to learn and test Reinforcement learning algorithms.\"\n",
    "\n",
    "\n",
    "### Task:\n",
    "To import the necessary packages, simply run the cell below, and then run the assertion cell to verify that they have been imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(gym)\n",
    "    print(\"Great, the packages were imported correctly!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Introduction\n",
    "\n",
    "The environment we are going to use is a simple grid world, where the agent is controlling a taxi. The environment was introduced in [Dietterich2000] to illustrate some issues in hierarchical reinforcement learning. There are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop the passenger off in another. You can read more about the environment here: https://gym.openai.com/envs/Taxi-v2/\n",
    "\n",
    "### Task:\n",
    "Create the environment variable containing all necessary methods to run the Taxi-v2 game. <br>\n",
    "\n",
    "_Tip: Just run the cell below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_id = \"Taxi-v2\"\n",
    "env = gym.make(environment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(env)\n",
    "    print(\"You successfully created the environement {}\".format(env.spec.id))\n",
    "except:\n",
    "    print(\"You create the environment incorrectly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build support methods for workshop\n",
    "Before you can go on, the cell below must be run. These are methods used to verify your work, in addition to the support function that will be used throughout the workshop. \n",
    "\n",
    "### Task\n",
    "Do as you have done before, simply mark the cell below and press CTRL + Enter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import gym\n",
    "import os\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class TestEnvironment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.reset()\n",
    "        \n",
    "    def play(self, action):\n",
    "        if not (0 <= int(action) <= 5):\n",
    "            print(\"Action value must either be 0,1,2,3,4 or 5\")\n",
    "            return\n",
    "\n",
    "        clear_output()\n",
    "        _, reward, done, _ = self.env.step(action)\n",
    "        self.env.render()\n",
    "        print(\"Reward: \", reward)\n",
    "        \n",
    "        if(done):\n",
    "            print(\"Game completed, resetting environment!\")\n",
    "            self.reset_env()\n",
    "        \n",
    "    def reset_env(self):\n",
    "        self.env.reset()\n",
    "        print(\"Environment has been reset\")\n",
    "        time.sleep(2)\n",
    "        clear_output()\n",
    "\n",
    "class MockData():\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(environment_id)\n",
    "        self.env.seed(10)\n",
    "        self.should_assert = True\n",
    "        \n",
    "    def get_Q(self):\n",
    "        Q = np.zeros([self.env.observation_space.n, self.env.action_space.n])\n",
    "        Q[0:2,0] = 10\n",
    "        return Q\n",
    "    \n",
    "    def get_env(self):\n",
    "        return self.env\n",
    "    \n",
    "    def turn_off_assertion(self):\n",
    "        self.should_assert = False\n",
    "        \n",
    "    def do_assertion(self):\n",
    "        return self.should_assert\n",
    "\n",
    "def visualize_q_table():\n",
    "    fig=plt.figure(figsize=(10, 10))\n",
    "    heat_map = sb.heatmap(Q_trained)\n",
    "    plt.show()\n",
    "\n",
    "def plot_visualization(data):\n",
    "    \n",
    "    reward_list = data[0]\n",
    "    iteration_list = data[1]\n",
    "    epsilon_list = data[2]\n",
    "    \n",
    "    episodes = range(len(reward_list))\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    ax = plt.subplot(311)\n",
    "    ax.set_title(\"Reward\")\n",
    "    ax = plt.plot(episodes, reward_list)\n",
    "\n",
    "    ax = plt.subplot(312)\n",
    "    ax.set_title(\"Iterations per episode\")\n",
    "    ax.plot(episodes, iteration_list)\n",
    "    \n",
    "    ax = plt.subplot(313)\n",
    "    ax.set_title(\"Epsilon\")\n",
    "    ax.set_xlabel(\"Episodes\")\n",
    "    ax.plot(episodes, epsilon_list)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def reset_env_and_update_params(env):\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    \n",
    "    return state, done, iterations, total_reward\n",
    "\n",
    "def render_performance(env, Q):\n",
    "    iterations = 0\n",
    "    total_reward = 0\n",
    "    t_sleep = 1.2\n",
    "    \n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    time.sleep(t_sleep)\n",
    "    clear_output()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(Q[state,:]) \n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        state =  new_state\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(t_sleep)\n",
    "        clear_output()\n",
    "        \n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "        \n",
    "        if (iterations >= 20): \n",
    "            done = True\n",
    "            print(\"Agent did not complete the episode within 20 iterations, train your agent better!\")\n",
    "            return\n",
    "    \n",
    "    print(\"Your agent completed the task using {} iterations, \\\n",
    "          and got a total reward of {}\".format(iterations, total_reward))\n",
    "    \n",
    "test_env = TestEnvironment()\n",
    "mock = MockData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "try:\n",
    "    assert(mock)\n",
    "    print(\"Great, the support methods were created!\")\n",
    "except:\n",
    "    print(\"You did not run the cell above, do this before you continue!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with the environment\n",
    "Before we start creating the Q-learning agent, we will explore the environment we are going to use. This is done using the support methods we have created for you.\n",
    "\n",
    "**How to play**\n",
    "1. Simply input an action, e.g. the integer 1, where it is indicated in the cell below, and click CTRL + Enter. \n",
    "2. The game will reset when you have picked up and then dropped off the passenger at the indicated location.\n",
    "3. If you would like to reset the test environment during the execution, run the cell which indicates this below. \n",
    "\n",
    "*Tip*: The bold colored letter shows where the passenger should be picked up, and the normal colored letter shows the drop-off spot.\n",
    "\n",
    "**Possible actions**\n",
    "0. Move down\n",
    "1. Move up\n",
    "2. Move right\n",
    "3. Move left\n",
    "4. Pick up passenger\n",
    "5. Drop off passenger\n",
    "\n",
    "### Task:\n",
    "Play around with the environment to understand the rewards and how the game dynamics work. When you feel confident with the environment,  move on to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to reset the environment\n",
    "test_env.reset_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to play the game\n",
    "\n",
    "# Input your action below\n",
    "action = 0\n",
    "# Input your action above\n",
    "\n",
    "test_env.play(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion point\n",
    "The game above is a simple environment, which is easy for humans to solve. However, if you were to use traditional programming, how would you solve it? \n",
    "\n",
    "### Task\n",
    "Use 5 minutes in the group to come up with a strategy, and create a quick draft!\n",
    "\n",
    "<img src=\"Images/Discussion.jpg\" alt=\"drawing\" width=\"400\" height=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "In Reinforcement learning an agent moves from a state $S_{t}$, to a new state $S_{t+1}$ by taking an action $A_{t}$, and receiving the reward $R_{t}$ as illustrated below. The agent will repeat this process for a defined amount of iterations, and this process as a whole is known as an episode. The goal of the Reinforcement learning agent is to maximize the total reward it can collect during an episode. To improve the performance, it learns from each state transition, i.e. move from $S_{t}$ to $S_{t+1}$. How it learns, is what separates the different Reinforcement learning algorithms.\n",
    "\n",
    "<img src=\"Images/Agent.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "In this notebook, we will work with one of the most iconic Reinforcement learning algorithms in its simplest form, namely Q-learning using Q-tables. The Q-tables holds the values of the strategy that gives us the highest reward,  according to the agent's knowledge. The table can be thought of as the brain of the agent. Each row in the table represents a state, and the columns represent the possible actions in that state. This will be explained further in the next section.\n",
    "\n",
    "A Q-value is a measure of the total expected reward the agent will receive if it chooses that action and always picks the action with the highest Q-value in the succeeding states, based on its current knowledge of the environment, i.e. the Q-table. Simply said, the higher the Q-value, the better the agent believes the action is.\n",
    "\n",
    "The Q-learning algorithm is used to update the Q-values for a given action $A_{t}$ in the state $S_{t}$, and is updated after each iteration. The equation is shown below.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "A key point to note in this equation is that we are updating the Q(s, a) by using the highest Q-value in the next state, $S_{t+1}$. This value is unknown, and it is therefore just the agent's estimate of that state. This makes it an optimization problem were we gradually adjust each Q-value as we receive a reward moving between the states.\n",
    "\n",
    "\n",
    "This might seem daunting at first, however, we will through this notebook break it into simple pieces, which will hopefully, make it easier to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Q-table\n",
    "The Q-table is the brain of the agent and stores the agent's current knowledge of the Q-values of each state. Each row represents a state, and the columns represent all possible actions in that state. As a result, our table will have the dimensions, (rows=number of states, columns=number of actions). An illustration of the table is shown below, although without the labels on the columns. \n",
    "\n",
    "<img src=\"Images/Taxi_matrix_initial.png\" alt=\"drawing\" width=\"500\" height=\"500\"/>\n",
    "\n",
    "### Task:\n",
    "Create and return a Q-table using numpy, where each cell is initialized to zeros:\n",
    "- Create the Q-table by using the numpy command: np.zeros(rows, columns). <br>\n",
    "    https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html\n",
    "- Use the following command to get the number of states in the environment:\n",
    "    - env.observation_space.n\n",
    "- Use the following command to get the number of possible actions. PS. all states have the same amount of actions.\n",
    "    - env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_table(env):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    action_size = env.action_space.n\n",
    "    observation_size = env.observation_space.n\n",
    "    \n",
    "    Q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    \n",
    "    \"Write code above\" \n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    assert(np.count_nonzero(mock.get_Q() == 0)), \"All values in Q-table should be zero\"\n",
    "    assert(create_q_table(mock.get_env()).shape == (mock.get_env().observation_space.n, mock.get_env().action_space.n)), \\\n",
    "    \"The dimensions are wrong\"\n",
    "    print(\"The Q-table was correctly built! \" +\n",
    "          \"It has {} rows, each representing a unique state, and {} columns, each representing an action for that state.\"\\\n",
    "          .format(mock.get_env().observation_space.n, mock.get_env().action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best action\n",
    "Before we implement the Q-learning algorithm, we will implement a method that, given a state, chooses the action with the highest Q-value.\n",
    "\n",
    "### Task\n",
    "Return the column index of the highest Q-value in the Q-table, given a state.\n",
    "- Tip: Use the argmax function from the numpy library. <br> https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_action(Q_table, state):\n",
    "    \n",
    "    \"Write code below\" \n",
    "    best_action =  np.argmax(Q_table[state,:]) \n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    assert(get_best_action(mock.get_Q(), 1) == 0), \"The method did not pick the action with the highest Q_value\"\n",
    "    print(\"The best action for the test state was chosen, excellent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select an action\n",
    "\n",
    "Initially, all the Q-values in the Q-table are set to zero. As we begin to move around the environment we can end up in a loop, such that we only explore a small part of the environment. This problem is so common its gotten a name; The Exploration vs Exploitation dilemma!\n",
    "\n",
    "One simple and very effective solution is to use a strategy called Epsilon-Greedy. The strategy makes the agent choose mostly random actions in the initial episodes,  which forces the agent to explore the environment. The strategy then gradually shifts the agent behavior towards making greedy choices, i.e. choosing the action with the highest Q-value. The parameter deciding the degree of random actions taken is called epsilon.\n",
    "\n",
    "### Task:\n",
    "Create a method that randomly picks an action IF a random value in the range [0,1] is less than epsilon, or ELSE picks the best action using the method we created earlier.   \n",
    "- To pick a random action, use env.action_space.sample()\n",
    "- To generate a random number with uniform probability in the range [0,1], use np.random.uniform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q_table, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    if epsilon > np.random.uniform():\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = get_best_action(Q_table, state)\n",
    "    \n",
    "    \"Write code above\"    \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    assert(select_action(mock.get_Q(), 1, 0) == 0), \\\n",
    "    \"Method should always return the same value for this state, since epsilon is 0\"\n",
    "    assert not (len(set([select_action(mock.get_Q(), 1, 1) for x in range(20)])) <= 2), \\\n",
    "    \"Method should not return identical values when a random action should be chosen\"\n",
    "    print(\"The correct actions were picked, great job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradually shift towards exploitation and reduce exploration of state-space\n",
    "\n",
    "To make sure we are gradually moving from exploration to exploitation, we need to reduce the possibility of choosing a random action. In other words, we need to reduce epsilon. This is done by multiplying it with a constant called epsilon decay.\n",
    "\n",
    "### Task:\n",
    "Create a method which reduces epsilon by multiplying itself with the defined constant epsilon decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_epsilon(epsilon):\n",
    "    epsilon_decay = 0.95\n",
    "    \n",
    "    \"Write code below\" \n",
    "    epsilon *= epsilon_decay\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    assert(update_epsilon(5) == 4.75), \"Given an input of 5, the output should have been 4.75\"\n",
    "    print(\"Epsilon was correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Find the highest Q_value for a given state\n",
    "\n",
    "Now it is finally time to begin building our Q-learning algorithm. The method we are going to create first is the part marked red in the equation below. This method returns the highest Q-value of the new state.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ {\\color{red}{\\underset{a}{max} \\  Q(s_{t+1},a)}} - Q(s_{t},a_{t})\\big]\n",
    "\\end{equation}\n",
    "$ \n",
    "\n",
    "\n",
    "### Task\n",
    "Return the highest Q-value for a given state, which is here called new_state.\n",
    "- Tip: Use the numpy.max function to retrieve the column (action) with the highest Q-value for the given row(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_highest_Q_value_in_state(Q_table, new_state):\n",
    "    \"Write code below\"\n",
    "    best_Q_value = np.max(Q_table[new_state,:])\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return best_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    assert(find_highest_Q_value_in_state(mock.get_Q(), 1) == 10), \\\n",
    "    \"The method did not pick the action with the highest Q_value\"\n",
    "    print(\"Perfect, the method returned the highest Q-value for that state!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate error  in Q-value\n",
    "The next method we are going to implement calculates how much we need to adjust our Q-value based on the reward we receive from moving from state $S_{t}$ to state $S_{t+1}$. In the intro to the Q-learning section, we defined the Q-value as the assumed expected total reward the agent will receive if it moves from state $S_{t}$ taking action $A_{t}$, and then always picks the action with the highest Q-value throughout the episode using the current Q-table. As a result, the the correct Q-value $Q(S_{t}, A_{t})$ can be defined as\n",
    "\n",
    "$\n",
    "Q(s_{t}, a_{t}) = r_{t} + \\underset{a}{max} \\  Q(s_{t+1},a)\n",
    "$\n",
    "\n",
    "Assuming $\\gamma$ is equal to one, we see that the statement marked red would become equal to 0, if our Q-value was perfect, thereby making no changes to the Q-value.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\ \\big[{\\color{red}{r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})}}\\big]\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "The parameter $\\gamma$ is connected to how the agent values immediate rewards compared to later rewards, however explanation of this is outside the scope of this notebook. \n",
    "\n",
    "### Task:\n",
    "Implement the part of the equation mark red.\n",
    "- Use the method we created in the previous section to find the highest_q_value_in_state\n",
    "- Use brackets notation to select the correct cell i.e. Q_table[Desired_state, desired_action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_in_Q_value(Q_table, state, action, reward, new_state):\n",
    "    \n",
    "    discount_factor = 0.95\n",
    "    \n",
    "    \"Write code below\"\n",
    "    highest_q_value_in_state = find_highest_Q_value_in_state(Q_table, new_state)\n",
    "    \n",
    "    error_in_Q_value = reward + discount_factor * highest_q_value_in_state - Q_table[state,action]\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return error_in_Q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    assert(calculate_error_in_Q_value(mock.get_Q(), 0, 0, 10, 1) == 9.5), \\\n",
    "    \"The method did not calculate the correct error value for the test sample\"\n",
    "    print(\"The error in Q-value have been calculated correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Q-table\n",
    "\n",
    "We are almost ready to complete our Q-learning algorithm. \n",
    "\n",
    "The only element not yet explained is the parameter $\\alpha$. $\\alpha$ is known as the learning rate and controls how quick we update the Q-table. For example, a learning rate of 0.1 indicates that we will only use 10% of the actual error calculated when updating the Q-table. This is because we calculate the Q-value by using the highest Q-value of the next state, which is just our best estimate so far and might be erroneous. If that state has not been visited yet, it would simply have the initialized value of the table, i.e. 0. By using the learning rate, we account for this uncertainty and get a less oscillating system which is proven to give better results. \n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "Q(s_{t},a_{t})^{new} \\leftarrow {\\color{red}{Q(s_{t}, a_{t}) + \\alpha \\ \\big[r_{t} + \\gamma \\ \\underset{a}{max} \\  Q(s_{t+1},a) - Q(s_{t},a_{t})\\big]}}\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "To implement the algorithm there is one factor which needs to be accounted for. If the episode has ended due to a terminal state, the last reward received will be the Q-value of that state, since there is no possible next state. As a result, if the done parameter is true (an input to the method below), we set the Q-value of that state-action pair equal to the reward.\n",
    "\n",
    "### Task\n",
    "Create a method which either returns\n",
    " - The Q-value of that state-action pair equal to the reward, IF it was the final episode. (Done is true)\n",
    " - ELSE updates the Q-table by following the Q-learning algorithm, using the methods we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(Q_table, state, action, done, reward, new_state):\n",
    "    \n",
    "    learning_rate = 0.8\n",
    "    \n",
    "    \"Write code below\"\n",
    "    if(done):\n",
    "        Q_table[state, action] = reward\n",
    "    else:\n",
    "        Q_table[state, action] = Q_table[state, action] + \\\n",
    "        learning_rate * calculate_error_in_Q_value(Q_table, state, action, reward, new_state)\n",
    "    \"Write code above\"\n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    assert(update_q_table(mock.get_Q(), 2, 1, 1, 10, 3)[2,1] == 10), \\\n",
    "    \"The updated Q-value when simulation was done was incorrect\"\n",
    "    assert(update_q_table(mock.get_Q(), 0, 0, 0, 10, 1)[0,0] == 17.6), \\\n",
    "    \"The updated Q-value when the episode was not done, was incorrect\"\n",
    "    print(\"Q-tabel have been correctly updated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take one step\n",
    "\n",
    "Now we are ready to create a method which makes our agent make a move in the environment.\n",
    "\n",
    "### Task\n",
    "Create a method which selects the best action, passes this to our environment, and returns the action, reward, done new_state variables.\n",
    "- To choose an action use the select_action method we created earlier\n",
    "- To make a step in the environment, use this method env.step(action)\n",
    "- env.step(action) return the following values accordingly: new_state, reward, done, state_transition_probability\n",
    "- Ignore the state_transition_probability, as this is not used in this environment\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(Q_table, env, state, epsilon):\n",
    "    \n",
    "    \"Write code below\"\n",
    "    action = select_action(Q_table, state, epsilon)\n",
    "    new_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "    \"Write code above\"\n",
    "    \n",
    "    return action, reward, done, new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not edit - Assertion cell #\n",
    "if mock.do_assertion():\n",
    "    mock_env1 = deepcopy(mock.get_env())\n",
    "    mock_env1.reset()\n",
    "    mock_env2 = deepcopy(mock_env1)\n",
    "\n",
    "    new_state, _, _, _ = mock_env1.step(0)\n",
    "    assert(step(mock.get_Q(), mock_env2, 0, 0) == (0, -1, False, new_state)), \\\n",
    "    \"Your environment did not return the correct values\"\n",
    "    print(\"Your step method seems to function properly, good job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train method\n",
    "\n",
    "Finally, we can create the method which will train our agent, i.e. update the Q-table.\n",
    "\n",
    "### Task\n",
    "In this method, you will enter your code at multiple places, but always between \"Write code below\" and \"Write code above\". \n",
    "- You will only input methods you have already created. \n",
    "- Remember to store the value in the right variable, i.e. the Q-table is stored in the variable Q_table\n",
    "- To see which variables the methods returns, look at your implementation in the previous cells.\n",
    "- PS. The following cells do not have assertion cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, num_episodes, max_iterations):\n",
    "    done = False\n",
    "    epsilon = 1\n",
    "    reward_list, iterations_list, epsilon_list = [], [], []\n",
    "    \n",
    "    \"Write code below\"\n",
    "    # Create Q-table\n",
    "    Q_table = create_q_table(env)\n",
    "    \"Write code above\"\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "\n",
    "        if (done and episode % 10 == 0): \n",
    "            print(\"Episode: {} | Iterations: {} | Total Reward: {} | Epsilon: {}\"\\\n",
    "                  .format(episode, iterations, total_reward, epsilon))\n",
    "        \n",
    "        state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "        \n",
    "        \"Write code below\"\n",
    "        # Update epsilon\n",
    "        epsilon = update_epsilon(epsilon)\n",
    "        \n",
    "        \"Write code above\"\n",
    "        \n",
    "        while not done:\n",
    "            \n",
    "            \"Write code below\"\n",
    "            \n",
    "            # Take step\n",
    "            action, reward, done, new_state = step(Q_table, env, state, epsilon)\n",
    "                        \n",
    "            # Update Q-table\n",
    "            Q_table = update_q_table(Q_table, state, action, done, reward, new_state)\n",
    "            \n",
    "            \"Write code above\"\n",
    "            \n",
    "            # Set the new state as the current state\n",
    "            state = new_state\n",
    "            \n",
    "            # Update episode information\n",
    "            iterations += 1\n",
    "            total_reward += reward\n",
    "            \n",
    "            # End episode if it has lasted for too many iterations\n",
    "            if (iterations >= max_iterations): done = True\n",
    "\n",
    "        reward_list.append(total_reward)\n",
    "        iterations_list.append(iterations)\n",
    "        epsilon_list.append(epsilon)\n",
    "        \n",
    "    return Q_table, (reward_list, iterations_list, epsilon_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create play method\n",
    "\n",
    "This method can be used if you want to test the performance of your agent, by only running one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(env, Q_table):\n",
    "    \n",
    "    state, done, iterations, total_reward = reset_env_and_update_params(env)\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        # Take step\n",
    "        action, reward, done, new_state = step(Q_table, env, state, 0)\n",
    "\n",
    "        # Set the new state as the current state\n",
    "        state = new_state\n",
    "\n",
    "        # Update episode information\n",
    "        iterations += 1\n",
    "        total_reward += reward\n",
    "\n",
    "        if (iterations >= 100): done = True\n",
    "    \n",
    "    print(\"Iterations: {} | Total Reward: {}\".format(iterations, total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time to put our methods to use!\n",
    "\n",
    "Congratulation, you have created every method need to train your Q-learning agent!\n",
    "\n",
    "### Task\n",
    "To train your agent, simply set a value for the maximum_episodes and maximum_iterations\n",
    "- Tip 1: Maximum_episodes should be less than 2000\n",
    "- Tip 2: Maximum iterations should be less than 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_episodes = 0\n",
    "maximum_iterations = 0\n",
    "\n",
    "Q_table_trained, data = train(env, maximum_episodes, maximum_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot key variables \n",
    "\n",
    "This method plots different parameters during training. As a result, the training cell above must be run before this one.\n",
    "\n",
    "### Task\n",
    "- Simply run the cell below to create the plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    assert(data)\n",
    "    plot_visualization(data)\n",
    "except:\n",
    "    print(\"You need to run the **train** cell before running this cell, as it uses the data from the training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play around for one round\n",
    "\n",
    "This method runs one episode, using your trained Q-table. This is a quick way to test the performance.\n",
    "\n",
    "### Task\n",
    "- Run the cell below to quickly see the performance of your agent. This method does not alter your Q_table, so you can run it as many times as you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(env, Q_table_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize an episode\n",
    "\n",
    "### Task\n",
    "This method works similarly to the one above, but visualizes the environment.\n",
    "- If you would like to end the simulation, click on the Kernal tab in the toolbar, and click \"Interrupt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_performance(env, Q_table_trained)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And we are done, almost!\n",
    "Now you have everything you need to play around with the environment and alter different parameters to see how it impacts our agent. Can you improve the parameters already chosen? Suggestions:\n",
    "- Increase the number of episodes and reduce iterations\n",
    "- Change epsilon decay to change the Epsilon-Greedy Strategy\n",
    "- Change the learning rate to make smaller or larger corrections to the Q-table per update\n",
    "- Change the discount factor $\\gamma$\n",
    "\n",
    "If you would like to create your own cells, do the following:\n",
    "1. Click on an existing cell\n",
    "2. Click the Esc - button\n",
    "3. Press \"b\" to add a cell below, press \"a\" the add a cell above, or double tap \"d\" to delete a cell\n",
    "\n",
    "### Task\n",
    "Run the cell below to turn off the assertion cells. <br>\n",
    "Have fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock.turn_off_assertion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to try a different environment?\n",
    "If you want to try a different environment suitable for Q-learning with tables, you can try this one:\n",
    "- FrozenLake-v0\n",
    "- You can read more about it here: https://gym.openai.com/envs/FrozenLake-v0/\n",
    "- Tip: This environment may require the agent to do a more thorough exploration.\n",
    "\n",
    "To use it, simply replace it with \"Taxi-v2\" in the cell where we created the env-variabel. \n",
    "Tip: Remember to rerun the cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
