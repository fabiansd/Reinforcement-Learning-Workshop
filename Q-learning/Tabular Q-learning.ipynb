{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q-learning \n",
    "Using the Bellman equation.\n",
    "\n",
    "This code will learn the value of states and use that as policy to solve the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bash: ./xvfb: No such file or directory\n",
      "env: DISPLAY=: 1\n"
     ]
    }
   ],
   "source": [
    "# XVFB will be launched if you run on a server\n",
    "from IPython.display import HTML\n",
    "import gym\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ./xvfb start\n",
    "    %env DISPLAY = : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'FrozenLake-v0'\n",
    "GAMMA = 0.9\n",
    "TEST_EPISODES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment info\n",
    "There is a 33% chance tha we will slip on a tile. This means that when attempting to walk from a to b, there is a 33% chance that we will slip to the left or right, relative to our target cell, and 33% chance to reach the actual target cell.\n",
    "\n",
    "We will get reward = 1 only when we reach the goal. This means that we need at least one full episode of reaching to goal to be able to learn meaningfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total actions: 4\n",
      "Total states: 16\n",
      "\n",
      "Action: 0\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Action: 1\n",
      "  (Down)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Action: 2\n",
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Action: 3\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "print('Total actions: {}'.format(env.action_space.n))\n",
    "print('Total states: {}\\n'.format(env.observation_space.n))\n",
    "env.reset()\n",
    "\n",
    "for i in range(env.action_space.n):\n",
    "    print('Action: {}'.format(i))\n",
    "    env.step(i)\n",
    "    env.render()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(ENV_NAME)\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "        # This table shows the reward for going from a state to a new state with action\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        \n",
    "        # This table shows the number of times the agent ended up in the new states when executing action in state\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        \n",
    "        self.values = collections.defaultdict(float)\n",
    "        \n",
    "        \n",
    "    def play_n_random_steps(self, count):\n",
    "        '''\n",
    "        This function is used to gather random experience from the env \n",
    "        and update the reward and transition tables\n",
    "        \n",
    "        parameters: \n",
    "            count: the number of random steps to take\n",
    "        '''\n",
    "        \n",
    "        for _ in range(count):\n",
    "            # Sample and perform random action\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, _ = self.env.step(action)\n",
    "            \n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            \n",
    "            if is_done:\n",
    "                self.state = self.env.reset()\n",
    "            else: \n",
    "                self.state = new_state\n",
    "                \n",
    "                \n",
    "    def calc_action_value(self, state, action):\n",
    "        '''\n",
    "        This function calculates the value of the action from the state using the transits, rewards, and values table.\n",
    "        it will be used for two purposes:\n",
    "         - Select the best action to perform from a state\n",
    "         - Calculate the new value of the state on value iteration\n",
    "         \n",
    "         The approximate the value of the state and action (Q(s,a) will be equal to the probability to of \n",
    "         every state, multiplied with the value of the sate)\n",
    "         \n",
    "         parameters:\n",
    "             state: agent's state in environment\n",
    "             action: agent's action in state\n",
    "             \n",
    "         return:\n",
    "             action_value: approximate value of state and action\n",
    "        '''\n",
    "        \n",
    "        # The key is state and action, and value is a count of experienced transitions\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        \n",
    "        # Calculate the total number of transits from this state with action\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        \n",
    "        #Iterate through every transit and calculate its contribution using Bellman equation\n",
    "        for target_state, count in target_counts.items():\n",
    "            reward = self.rewards[(state, action, target_state)]\n",
    "            \n",
    "            # This is the empirical probability of ending up in target_state when performing action in state\n",
    "            p = (count / total)\n",
    "            action_value += p * (reward + GAMMA * self.values[target_state])\n",
    "            \n",
    "        return action_value\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        '''\n",
    "        This function uses calc_action_value to make a decision about the best action \n",
    "        to take from the given state\n",
    "        parameter:\n",
    "            state: state to calculate best action from\n",
    "        return:\n",
    "            best_action: the approximate best action to take from state\n",
    "        '''\n",
    "        \n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value == None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "                \n",
    "        return best_action\n",
    "    \n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        '''\n",
    "        This function uses select_action to get best action from state and perform it in the provided environment.\n",
    "        It is used to play test episodes, during which we don't want to mess with the state of our main environment\n",
    "        used to gather random data.\n",
    "        parameters:\n",
    "            env: second environment used for test episodes\n",
    "        return:\n",
    "            total_reward: accumulated reward for the episode\n",
    "        '''\n",
    "        total_reward = 0.0\n",
    "        \n",
    "        state = env.reset()\n",
    "\n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, is_done, _ = env.step(action)\n",
    "            self.rewards[(state, action, new_state)] = reward\n",
    "            self.transits[(state, action)][new_state] += 1\n",
    "            total_reward += reward\n",
    "            \n",
    "            if is_done: break\n",
    "            state = new_state\n",
    "            \n",
    "        return total_reward\n",
    "    \n",
    "    \n",
    "    def value_iteration(self):\n",
    "        '''\n",
    "        This function calculates the action values using the experience data thorugh value iteration.\n",
    "        Value iteration is essentially:\n",
    "         1 Initialize all action values (Q) to 0\n",
    "         2 For every state and action in this state, perform the calc_action_value update\n",
    "         3 Repeat step 2 for som large number of steps until changes become too small\n",
    "        '''\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            # Calculate action_value for each action in state\n",
    "            state_values = [self.calc_action_value(state, action) for action in range(self.env.action_space.n)]\n",
    "            # Select action with highest action_value as state value \n",
    "            self.values[state] = max(state_values)\n",
    "            \n",
    "            \n",
    "    def gen_test_movie(self, env):\n",
    "        \n",
    "        env.reset()\n",
    "        \n",
    "        env_monitor = gym.wrappers.Monitor(env, directory=ENV_NAME, force=True)\n",
    "\n",
    "        #Why is three simulations needed for saving?!\n",
    "        for _ in range(3):\n",
    "            done = False\n",
    "            env_monitor.reset()\n",
    "            while not done:\n",
    "                _, _, done, _ = env_monitor.step(env_atari.action_space.sample())\n",
    "\n",
    "        env_atari_monitor.close()\n",
    "        env_atari.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "    def print_info(self):\n",
    "        print('rewards table: (state, action, new-state) : reward)')\n",
    "        print(self.rewards)\n",
    "        print()\n",
    "        print('transits table: (state, action) : {new-states: count})')\n",
    "        print(self.transits)\n",
    "        print()\n",
    "        print('values table')\n",
    "        print(self.values)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best reward updated: 0.05\n",
      "best reward updated: 0.25\n",
      "best reward updated: 0.35\n",
      "best reward updated: 0.45\n",
      "best reward updated: 0.55\n",
      "best reward updated: 0.85\n",
      "Solved in 17 iterations\n"
     ]
    }
   ],
   "source": [
    "test_env = gym.make(ENV_NAME)\n",
    "agent = Agent()\n",
    "writer = SummaryWriter(comment=\"-v-learning\")\n",
    "\n",
    "iter_no = 0\n",
    "best_reward = 0.0\n",
    "\n",
    "'''\n",
    "First, we perform 100 random steps to fill our reward and transition tables with \n",
    "fresh data and then we eun value iteration over all states. Then we play test episodes\n",
    "using the value table as policy.\n",
    "'''\n",
    "\n",
    "while True:\n",
    "    iter_no += 1\n",
    "    agent.play_n_random_steps(100)\n",
    "    agent.value_iteration()\n",
    "    \n",
    "    reward = 0.0\n",
    "    for _ in range(TEST_EPISODES):\n",
    "        reward += agent.play_episode(test_env)\n",
    "    reward /= TEST_EPISODES\n",
    "    writer.add_scalar(\"reward\", reward, iter_no)\n",
    "    \n",
    "    if reward > best_reward:\n",
    "        print('best reward updated: {}'.format(reward))\n",
    "        best_reward = reward\n",
    "    \n",
    "    if reward > 0.8:\n",
    "        print('Solved in {} iterations'.format(iter_no))\n",
    "        break\n",
    "        \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
